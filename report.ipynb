{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from model import BertEmbeddings\n",
    "from model import BertAttention\n",
    "from model import BertLayer\n",
    "from model import BertEncoder\n",
    "from model import BertModel\n",
    "from model import BertConfig\n",
    "import torch\n",
    "from utils.log_helper import logger_init"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preprocessing Principles\n",
    "In single-sentence classification, data preprocessing transforms raw text into model-ready tensor inputs. Key principles include:\n",
    "\n",
    "1. **Tokenization**: Use BERT’s WordPiece tokenizer to split sentences into subword tokens; out-of-vocabulary words are marked as `[UNK]` to maintain a fixed vocabulary size."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['青', '山', '不', '改', '，', '绿', '水', '长', '流', '，', '我', '们', '月', '来', '客', '栈', '见', '！']\n",
      "['10', '年', '前', '的', '今', '天', '，', '纪', '念', '5', '.', '12', '汶', '川', '大', '地', '震', '10', '周', '年']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from Tasks.TaskForSingleSentenceClassification import ModelConfig\n",
    "\n",
    "model_config = ModelConfig()\n",
    "tokenizer = BertTokenizer.from_pretrained(model_config.pretrained_model_dir).tokenize\n",
    "print(tokenizer(\"青山不改，绿水长流，我们月来客栈见！\"))\n",
    "print(tokenizer(\"10年前的今天，纪念5.12汶川大地震10周年\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "**I used bert-base-chinese model from HuggingFace,This model has been pre-trained for Chinese, training and random input masking has been applied independently to word pieces (as in the original BERT paper).Here is a glimpse at the dictionnary:{word:index}**\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[PAD]': 0, '[unused1]': 1, '[unused2]': 2, '[unused3]': 3, '[unused4]': 4, '[unused5]': 5, '[unused6]': 6, '[unused7]': 7, '[unused8]': 8, '[unused9]': 9, '[unused10]': 10, '[unused11]': 11, '[unused12]': 12, '[unused13]': 13, '[unused14]': 14, '[unused15]': 15, '[unused16]': 16, '[unused17]': 17, '[unused18]': 18, '[unused19]': 19, '[unused20]': 20, '[unused21]': 21, '[unused22]': 22, '[unused23]': 23, '[unused24]': 24, '[unused25]': 25, '[unused26]': 26, '[unused27]': 27, '[unused28]': 28, '[unused29]': 29, '[unused30]': 30, '[unused31]': 31, '[unused32]': 32, '[unused33]': 33, '[unused34]': 34, '[unused35]': 35, '[unused36]': 36, '[unused37]': 37, '[unused38]': 38, '[unused39]': 39, '[unused40]': 40, '[unused41]': 41, '[unused42]': 42, '[unused43]': 43, '[unused44]': 44, '[unused45]': 45, '[unused46]': 46, '[unused47]': 47, '[unused48]': 48, '[unused49]': 49, '[unused50]': 50, '[unused51]': 51, '[unused52]': 52, '[unused53]': 53, '[unused54]': 54, '[unused55]': 55, '[unused56]': 56, '[unused57]': 57, '[unused58]': 58, '[unused59]': 59, '[unused60]': 60, '[unused61]': 61, '[unused62]': 62, '[unused63]': 63, '[unused64]': 64, '[unused65]': 65, '[unused66]': 66, '[unused67]': 67, '[unused68]': 68, '[unused69]': 69, '[unused70]': 70, '[unused71]': 71, '[unused72]': 72, '[unused73]': 73, '[unused74]': 74, '[unused75]': 75, '[unused76]': 76, '[unused77]': 77, '[unused78]': 78, '[unused79]': 79, '[unused80]': 80, '[unused81]': 81, '[unused82]': 82, '[unused83]': 83, '[unused84]': 84, '[unused85]': 85, '[unused86]': 86, '[unused87]': 87, '[unused88]': 88, '[unused89]': 89, '[unused90]': 90, '[unused91]': 91, '[unused92]': 92, '[unused93]': 93, '[unused94]': 94, '[unused95]': 95, '[unused96]': 96, '[unused97]': 97, '[unused98]': 98, '[unused99]': 99, '[UNK]': 100, '[CLS]': 101, '[SEP]': 102, '[MASK]': 103, '<S>': 104, '<T>': 105, '!': 106, '\"': 107, '#': 108, '$': 109, '%': 110, '&': 111, \"'\": 112, '(': 113, ')': 114, '*': 115, '+': 116, ',': 117, '-': 118, '.': 119, '/': 120, '0': 121, '1': 122, '2': 123, '3': 124, '4': 125, '5': 126, '6': 127, '7': 128, '8': 129, '9': 130, ':': 131, ';': 132, '<': 133, '=': 134, '>': 135, '?': 136, '@': 137, '[': 138, '\\\\': 139, ']': 140, '^': 141, '_': 142, 'a': 143, 'b': 144, 'c': 145, 'd': 146, 'e': 147, 'f': 148, 'g': 149, 'h': 150, 'i': 151, 'j': 152, 'k': 153, 'l': 154, 'm': 155, 'n': 156, 'o': 157, 'p': 158, 'q': 159, 'r': 160, 's': 161, 't': 162, 'u': 163, 'v': 164, 'w': 165, 'x': 166, 'y': 167, 'z': 168, '{': 169, '|': 170, '}': 171, '~': 172, '£': 173, '¤': 174, '¥': 175, '§': 176, '©': 177, '«': 178, '®': 179, '°': 180, '±': 181, '²': 182, '³': 183, 'µ': 184, '·': 185, '¹': 186, 'º': 187, '»': 188, '¼': 189, '×': 190, 'ß': 191, 'æ': 192, '÷': 193, 'ø': 194, 'đ': 195, 'ŋ': 196, 'ɔ': 197, 'ə': 198, 'ɡ': 199, 'ʰ': 200, 'ˇ': 201, 'ˈ': 202, 'ˊ': 203, 'ˋ': 204, 'ˍ': 205, 'ː': 206, '˙': 207, '˚': 208, 'ˢ': 209, 'α': 210, 'β': 211, 'γ': 212, 'δ': 213, 'ε': 214, 'η': 215, 'θ': 216, 'ι': 217, 'κ': 218, 'λ': 219, 'μ': 220, 'ν': 221, 'ο': 222, 'π': 223, 'ρ': 224, 'ς': 225, 'σ': 226, 'τ': 227, 'υ': 228, 'φ': 229, 'χ': 230, 'ψ': 231, 'ω': 232, 'а': 233, 'б': 234, 'в': 235, 'г': 236, 'д': 237, 'е': 238, 'ж': 239, 'з': 240, 'и': 241, 'к': 242, 'л': 243, 'м': 244, 'н': 245, 'о': 246, 'п': 247, 'р': 248, 'с': 249, 'т': 250, 'у': 251, 'ф': 252, 'х': 253, 'ц': 254, 'ч': 255, 'ш': 256, 'ы': 257, 'ь': 258, 'я': 259, 'і': 260, 'ا': 261, 'ب': 262, 'ة': 263, 'ت': 264, 'د': 265, 'ر': 266, 'س': 267, 'ع': 268, 'ل': 269, 'م': 270, 'ن': 271, 'ه': 272, 'و': 273, 'ي': 274, '۩': 275, 'ก': 276, 'ง': 277, 'น': 278, 'ม': 279, 'ย': 280, 'ร': 281, 'อ': 282, 'า': 283, 'เ': 284, '๑': 285, '་': 286, 'ღ': 287, 'ᄀ': 288, 'ᄁ': 289, 'ᄂ': 290, 'ᄃ': 291, 'ᄅ': 292, 'ᄆ': 293, 'ᄇ': 294, 'ᄈ': 295, 'ᄉ': 296, 'ᄋ': 297, 'ᄌ': 298, 'ᄎ': 299, 'ᄏ': 300, 'ᄐ': 301, 'ᄑ': 302, 'ᄒ': 303, 'ᅡ': 304, 'ᅢ': 305, 'ᅣ': 306, 'ᅥ': 307, 'ᅦ': 308, 'ᅧ': 309, 'ᅨ': 310, 'ᅩ': 311, 'ᅪ': 312, 'ᅬ': 313, 'ᅭ': 314, 'ᅮ': 315, 'ᅯ': 316, 'ᅲ': 317, 'ᅳ': 318, 'ᅴ': 319, 'ᅵ': 320, 'ᆨ': 321, 'ᆫ': 322, 'ᆯ': 323, 'ᆷ': 324, 'ᆸ': 325, 'ᆺ': 326, 'ᆻ': 327, 'ᆼ': 328, 'ᗜ': 329, 'ᵃ': 330, 'ᵉ': 331, 'ᵍ': 332, 'ᵏ': 333, 'ᵐ': 334, 'ᵒ': 335, 'ᵘ': 336, '‖': 337, '„': 338, '†': 339, '•': 340, '‥': 341, '‧': 342, '\\u2028': 343, '‰': 344, '′': 345, '″': 346, '‹': 347, '›': 348, '※': 349, '‿': 350, '⁄': 351, 'ⁱ': 352, '⁺': 353, 'ⁿ': 354, '₁': 355, '₂': 356, '₃': 357, '₄': 358, '€': 359, '℃': 360, '№': 361, '™': 362, 'ⅰ': 363, 'ⅱ': 364, 'ⅲ': 365, 'ⅳ': 366, 'ⅴ': 367, '←': 368, '↑': 369, '→': 370, '↓': 371, '↔': 372, '↗': 373, '↘': 374, '⇒': 375, '∀': 376, '−': 377, '∕': 378, '∙': 379, '√': 380, '∞': 381, '∟': 382, '∠': 383, '∣': 384, '∥': 385, '∩': 386, '∮': 387, '∶': 388, '∼': 389, '∽': 390, '≈': 391, '≒': 392, '≡': 393, '≤': 394, '≥': 395, '≦': 396, '≧': 397, '≪': 398, '≫': 399, '⊙': 400, '⋅': 401, '⋈': 402, '⋯': 403, '⌒': 404, '①': 405, '②': 406, '③': 407, '④': 408, '⑤': 409, '⑥': 410, '⑦': 411, '⑧': 412, '⑨': 413, '⑩': 414, '⑴': 415, '⑵': 416, '⑶': 417, '⑷': 418, '⑸': 419, '⒈': 420, '⒉': 421, '⒊': 422, '⒋': 423, 'ⓒ': 424, 'ⓔ': 425, 'ⓘ': 426, '─': 427, '━': 428, '│': 429, '┃': 430, '┅': 431, '┆': 432, '┊': 433, '┌': 434, '└': 435, '├': 436, '┣': 437, '═': 438, '║': 439, '╚': 440, '╞': 441, '╠': 442, '╭': 443, '╮': 444, '╯': 445, '╰': 446, '╱': 447, '╳': 448, '▂': 449, '▃': 450, '▅': 451, '▇': 452, '█': 453, '▉': 454, '▋': 455, '▌': 456, '▍': 457, '▎': 458, '■': 459, '□': 460, '▪': 461, '▫': 462, '▬': 463, '▲': 464, '△': 465, '▶': 466, '►': 467, '▼': 468, '▽': 469, '◆': 470, '◇': 471, '○': 472, '◎': 473, '●': 474, '◕': 475, '◠': 476, '◢': 477, '◤': 478, '☀': 479, '★': 480, '☆': 481, '☕': 482, '☞': 483, '☺': 484, '☼': 485, '♀': 486, '♂': 487, '♠': 488, '♡': 489, '♣': 490, '♥': 491, '♦': 492, '♪': 493, '♫': 494, '♬': 495, '✈': 496, '✔': 497, '✕': 498, '✖': 499, '✦': 500, '✨': 501, '✪': 502, '✰': 503, '✿': 504, '❀': 505, '❤': 506, '➜': 507, '➤': 508, '⦿': 509, '、': 510, '。': 511, '〃': 512, '々': 513, '〇': 514, '〈': 515, '〉': 516, '《': 517, '》': 518, '「': 519, '」': 520, '『': 521, '』': 522, '【': 523, '】': 524, '〓': 525, '〔': 526, '〕': 527, '〖': 528, '〗': 529, '〜': 530, '〝': 531, '〞': 532, 'ぁ': 533, 'あ': 534, 'ぃ': 535, 'い': 536, 'う': 537, 'ぇ': 538, 'え': 539, 'お': 540, 'か': 541, 'き': 542, 'く': 543, 'け': 544, 'こ': 545, 'さ': 546, 'し': 547, 'す': 548, 'せ': 549, 'そ': 550, 'た': 551, 'ち': 552, 'っ': 553, 'つ': 554, 'て': 555, 'と': 556, 'な': 557, 'に': 558, 'ぬ': 559, 'ね': 560, 'の': 561, 'は': 562, 'ひ': 563, 'ふ': 564, 'へ': 565, 'ほ': 566, 'ま': 567, 'み': 568, 'む': 569, 'め': 570, 'も': 571, 'ゃ': 572, 'や': 573, 'ゅ': 574, 'ゆ': 575, 'ょ': 576, 'よ': 577, 'ら': 578, 'り': 579, 'る': 580, 'れ': 581, 'ろ': 582, 'わ': 583, 'を': 584, 'ん': 585, '゜': 586, 'ゝ': 587, 'ァ': 588, 'ア': 589, 'ィ': 590, 'イ': 591, 'ゥ': 592, 'ウ': 593, 'ェ': 594, 'エ': 595, 'ォ': 596, 'オ': 597, 'カ': 598, 'キ': 599, 'ク': 600, 'ケ': 601, 'コ': 602, 'サ': 603, 'シ': 604, 'ス': 605, 'セ': 606, 'ソ': 607, 'タ': 608, 'チ': 609, 'ッ': 610, 'ツ': 611, 'テ': 612, 'ト': 613, 'ナ': 614, 'ニ': 615, 'ヌ': 616, 'ネ': 617, 'ノ': 618, 'ハ': 619, 'ヒ': 620, 'フ': 621, 'ヘ': 622, 'ホ': 623, 'マ': 624, 'ミ': 625, 'ム': 626, 'メ': 627, 'モ': 628, 'ャ': 629, 'ヤ': 630, 'ュ': 631, 'ユ': 632, 'ョ': 633, 'ヨ': 634, 'ラ': 635, 'リ': 636, 'ル': 637, 'レ': 638, 'ロ': 639, 'ワ': 640, 'ヲ': 641, 'ン': 642, 'ヶ': 643, '・': 644, 'ー': 645, 'ヽ': 646, 'ㄅ': 647, 'ㄆ': 648, 'ㄇ': 649, 'ㄉ': 650, 'ㄋ': 651, 'ㄌ': 652, 'ㄍ': 653, 'ㄎ': 654, 'ㄏ': 655, 'ㄒ': 656, 'ㄚ': 657, 'ㄛ': 658, 'ㄞ': 659, 'ㄟ': 660, 'ㄢ': 661, 'ㄤ': 662, 'ㄥ': 663, 'ㄧ': 664, 'ㄨ': 665, 'ㆍ': 666, '㈦': 667, '㊣': 668, '㎡': 669, '㗎': 670, '一': 671, '丁': 672, '七': 673, '万': 674, '丈': 675, '三': 676, '上': 677, '下': 678, '不': 679, '与': 680, '丐': 681, '丑': 682, '专': 683, '且': 684, '丕': 685, '世': 686, '丘': 687, '丙': 688, '业': 689, '丛': 690, '东': 691, '丝': 692, '丞': 693, '丟': 694, '両': 695, '丢': 696, '两': 697, '严': 698, '並': 699, '丧': 700, '丨': 701, '个': 702, '丫': 703, '中': 704, '丰': 705, '串': 706, '临': 707, '丶': 708, '丸': 709, '丹': 710, '为': 711, '主': 712, '丼': 713, '丽': 714, '举': 715, '丿': 716, '乂': 717, '乃': 718, '久': 719, '么': 720, '义': 721, '之': 722, '乌': 723, '乍': 724, '乎': 725, '乏': 726, '乐': 727, '乒': 728, '乓': 729, '乔': 730, '乖': 731, '乗': 732, '乘': 733, '乙': 734, '乜': 735, '九': 736, '乞': 737, '也': 738, '习': 739, '乡': 740, '书': 741, '乩': 742, '买': 743, '乱': 744, '乳': 745, '乾': 746, '亀': 747, '亂': 748, '了': 749, '予': 750, '争': 751, '事': 752, '二': 753, '于': 754, '亏': 755, '云': 756, '互': 757, '五': 758, '井': 759, '亘': 760, '亙': 761, '亚': 762, '些': 763, '亜': 764, '亞': 765, '亟': 766, '亡': 767, '亢': 768, '交': 769, '亥': 770, '亦': 771, '产': 772, '亨': 773, '亩': 774, '享': 775, '京': 776, '亭': 777, '亮': 778, '亲': 779, '亳': 780, '亵': 781, '人': 782, '亿': 783, '什': 784, '仁': 785, '仃': 786, '仄': 787, '仅': 788, '仆': 789, '仇': 790, '今': 791, '介': 792, '仍': 793, '从': 794, '仏': 795, '仑': 796, '仓': 797, '仔': 798, '仕': 799, '他': 800, '仗': 801, '付': 802, '仙': 803, '仝': 804, '仞': 805, '仟': 806, '代': 807, '令': 808, '以': 809, '仨': 810, '仪': 811, '们': 812, '仮': 813, '仰': 814, '仲': 815, '件': 816, '价': 817, '任': 818, '份': 819, '仿': 820, '企': 821, '伉': 822, '伊': 823, '伍': 824, '伎': 825, '伏': 826, '伐': 827, '休': 828, '伕': 829, '众': 830, '优': 831, '伙': 832, '会': 833, '伝': 834, '伞': 835, '伟': 836, '传': 837, '伢': 838, '伤': 839, '伦': 840, '伪': 841, '伫': 842, '伯': 843, '估': 844, '伴': 845, '伶': 846, '伸': 847, '伺': 848, '似': 849, '伽': 850, '佃': 851, '但': 852, '佇': 853, '佈': 854, '位': 855, '低': 856, '住': 857, '佐': 858, '佑': 859, '体': 860, '佔': 861, '何': 862, '佗': 863, '佘': 864, '余': 865, '佚': 866, '佛': 867, '作': 868, '佝': 869, '佞': 870, '佟': 871, '你': 872, '佢': 873, '佣': 874, '佤': 875, '佥': 876, '佩': 877, '佬': 878, '佯': 879, '佰': 880, '佳': 881, '併': 882, '佶': 883, '佻': 884, '佼': 885, '使': 886, '侃': 887, '侄': 888, '來': 889, '侈': 890, '例': 891, '侍': 892, '侏': 893, '侑': 894, '侖': 895, '侗': 896, '供': 897, '依': 898, '侠': 899, '価': 900, '侣': 901, '侥': 902, '侦': 903, '侧': 904, '侨': 905, '侬': 906, '侮': 907, '侯': 908, '侵': 909, '侶': 910, '侷': 911, '便': 912, '係': 913, '促': 914, '俄': 915, '俊': 916, '俎': 917, '俏': 918, '俐': 919, '俑': 920, '俗': 921, '俘': 922, '俚': 923, '保': 924, '俞': 925, '俟': 926, '俠': 927, '信': 928, '俨': 929, '俩': 930, '俪': 931, '俬': 932, '俭': 933, '修': 934, '俯': 935, '俱': 936, '俳': 937, '俸': 938, '俺': 939, '俾': 940, '倆': 941, '倉': 942, '個': 943, '倌': 944, '倍': 945, '倏': 946, '們': 947, '倒': 948, '倔': 949, '倖': 950, '倘': 951, '候': 952, '倚': 953, '倜': 954, '借': 955, '倡': 956, '値': 957, '倦': 958, '倩': 959, '倪': 960, '倫': 961, '倬': 962, '倭': 963, '倶': 964, '债': 965, '值': 966, '倾': 967, '偃': 968, '假': 969, '偈': 970, '偉': 971, '偌': 972, '偎': 973, '偏': 974, '偕': 975, '做': 976, '停': 977, '健': 978, '側': 979, '偵': 980, '偶': 981, '偷': 982, '偻': 983, '偽': 984, '偿': 985, '傀': 986, '傅': 987, '傍': 988, '傑': 989, '傘': 990, '備': 991, '傚': 992, '傢': 993, '傣': 994, '傥': 995, '储': 996, '傩': 997, '催': 998, '傭': 999}\n"
     ]
    }
   ],
   "source": [
    "from utils.data_helpers import Vocab,build_vocab\n",
    "import itertools\n",
    "vocab=build_vocab(\"bert_base_chinese/vocab.txt\")\n",
    "def get_first_100_items(dictionary):\n",
    "    return dict(itertools.islice(dictionary.items(), 1000))\n",
    "\n",
    "example_dict = {i: f\"value_{i}\" for i in range(200)}\n",
    "first_1000_items = get_first_100_items(vocab.stoi)\n",
    "print(first_1000_items)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. **Special Tokens**: Prepend `[CLS]`(with index 101) at the beginning of each sequence to aggregate sentence-level features, and append `[SEP]`(with index 102) at the end to signal input boundaries.\n",
    "3. **Token-to-ID Mapping**: Convert each token into its corresponding integer `token_id` using the vocabulary (`vocab.txt`), forming the input ID sequence."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer\n",
    "from Tasks.TaskForSingleSentenceClassification import BertConfig\n",
    "\n",
    "class Load1SingleSentenceClassificationDataset:\n",
    "    def __init__(self,\n",
    "                 vocab_path='bert_base_chinese/vocab.txt',  #\n",
    "                 tokenizer=None,\n",
    "                 batch_size=32,\n",
    "                 max_sen_len=None,\n",
    "                 split_sep='_!_',\n",
    "                 max_position_embeddings=512,\n",
    "                 pad_index=0,\n",
    "                 is_sample_shuffle=True\n",
    "                 ):\n",
    "        model_config = ModelConfig()\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_config.pretrained_model_dir).tokenize\n",
    "        self.vocab = build_vocab(vocab_path)\n",
    "        self.PAD_IDX = pad_index\n",
    "        self.SEP_IDX = self.vocab['[SEP]']\n",
    "        self.CLS_IDX = self.vocab['[CLS]']\n",
    "        self.batch_size = batch_size\n",
    "        self.split_sep = split_sep\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        if isinstance(max_sen_len, int) and max_sen_len > max_position_embeddings:\n",
    "            max_sen_len = max_position_embeddings\n",
    "        self.max_sen_len = max_sen_len\n",
    "        self.is_sample_shuffle = is_sample_shuffle\n",
    "    def data_process(self, file_path=None):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        raw_iter = open(file_path, encoding=\"utf8\").readlines()\n",
    "        data = []\n",
    "        max_len = 0\n",
    "        for raw in tqdm(raw_iter, ncols=80):\n",
    "            line = raw.rstrip(\"\\n\").split(self.split_sep)\n",
    "            print(line)\n",
    "            s, l = line[0], line[1]\n",
    "            tmp = [self.CLS_IDX] + [self.vocab[token] for token in self.tokenizer(s)]\n",
    "            if len(tmp) > self.max_position_embeddings - 1:\n",
    "                tmp = tmp[:self.max_position_embeddings - 1]  # BERT预训练模型只取前512个字符\n",
    "            tmp += [self.SEP_IDX]\n",
    "            tensor_ = torch.tensor(tmp, dtype=torch.long)\n",
    "            l = torch.tensor(int(l), dtype=torch.long)\n",
    "            max_len = max(max_len, tensor_.size(0))\n",
    "            data.append((tensor_))\n",
    "        return data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 6/6 [00:00<00:00, 2997.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['张艺兴黄金瞳片场，导演能给个合适的帽子不？', '2']\n",
      "['故宫如何修文物？文物医院下月向公众开放', '1']\n",
      "['深圳房价是沈阳6倍就是因为经济？错！', '5']\n",
      "['不负春光，樱花树下；温暖你我，温暖龙岩', '10']\n",
      "['二胡，如何对？', '2']\n",
      "['轻松一刻：带你看全球最噩梦监狱，每天进几百人，审讯时已过几年', '11']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "[tensor([ 101, 2476, 5686, 1069, 7942, 7032, 4749, 4275, 1767, 8024, 2193, 4028,\n         5543, 5314,  702, 1394, 6844, 4638, 2384, 2094,  679, 8043,  102]),\n tensor([ 101, 3125, 2151, 1963,  862,  934, 3152, 4289, 8043, 3152, 4289, 1278,\n         7368,  678, 3299, 1403, 1062,  830, 2458, 3123,  102]),\n tensor([ 101, 3918, 1766, 2791,  817, 3221, 3755, 7345,  127,  945, 2218, 3221,\n         1728,  711, 5307, 3845, 8043, 7231, 8013,  102]),\n tensor([ 101,  679, 6566, 3217, 1045, 8024, 3569, 5709, 3409,  678, 8039, 3946,\n         3265,  872, 2769, 8024, 3946, 3265, 7987, 2272,  102]),\n tensor([ 101,  753, 5529, 8024, 1963,  862, 2190, 8043,  102]),\n tensor([ 101, 6768, 3351,  671, 1174, 8038, 2372,  872, 4692, 1059, 4413, 3297,\n         1691, 3457, 4664, 4328, 8024, 3680, 1921, 6822, 1126, 4636,  782, 8024,\n         2144, 6380, 3198, 2347, 6814, 1126, 2399,  102])]"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_class=Load1SingleSentenceClassificationDataset()\n",
    "data=sentence_class.data_process(\"data/SingleSentenceClassification/jupyter_test.txt\")\n",
    "data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. **Truncation & Padding & Padding Mask**:\n",
    "   - **Truncation**: If the sequence length exceeds `max_position_embeddings - 1`, truncate to fit the position embedding limit.\n",
    "   - **Padding**: If the sequence is shorter, pad with `pad_index` tokens so that all sequences in a batch share the same length.\n",
    "   - **Padding Mask**:After obtaining `input_ids`, generate a binary mask `attention_mask = (input_ids == loader.PAD_IDX)` where `1` indicates padded positions (to be masked out) and `0` indicates valid tokens. This mask ensures the model ignores padding during attention computations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def pad_sequence(sequences, batch_first=False, max_len=None, padding_value=0):\n",
    "    if max_len is None:\n",
    "        max_len = max([s.size(0) for s in sequences])\n",
    "    out_tensors = []\n",
    "    for tensor in sequences:\n",
    "        if tensor.size(0) < max_len:\n",
    "            tensor = torch.cat([tensor, torch.tensor([padding_value] * (max_len - tensor.size(0)))], dim=0)\n",
    "        else:\n",
    "            tensor = tensor[:max_len]\n",
    "        out_tensors.append(tensor)\n",
    "    out_tensors = torch.stack(out_tensors, dim=1)\n",
    "    if batch_first:\n",
    "        return out_tensors.transpose(0, 1)\n",
    "    return out_tensors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 101, 2476, 5686, 1069, 7942, 7032, 4749, 4275, 1767, 8024, 2193, 4028,\n         5543, 5314,  702, 1394, 6844, 4638, 2384, 2094,  679, 8043,  102,    0,\n            0,    0,    0,    0,    0,    0,    0,    0],\n        [ 101, 3125, 2151, 1963,  862,  934, 3152, 4289, 8043, 3152, 4289, 1278,\n         7368,  678, 3299, 1403, 1062,  830, 2458, 3123,  102,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0],\n        [ 101, 3918, 1766, 2791,  817, 3221, 3755, 7345,  127,  945, 2218, 3221,\n         1728,  711, 5307, 3845, 8043, 7231, 8013,  102,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0],\n        [ 101,  679, 6566, 3217, 1045, 8024, 3569, 5709, 3409,  678, 8039, 3946,\n         3265,  872, 2769, 8024, 3946, 3265, 7987, 2272,  102,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0],\n        [ 101,  753, 5529, 8024, 1963,  862, 2190, 8043,  102,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0],\n        [ 101, 6768, 3351,  671, 1174, 8038, 2372,  872, 4692, 1059, 4413, 3297,\n         1691, 3457, 4664, 4328, 8024, 3680, 1921, 6822, 1126, 4636,  782, 8024,\n         2144, 6380, 3198, 2347, 6814, 1126, 2399,  102]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad=pad_sequence(data, max_len=None).transpose(1,0)\n",
    "pad"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n",
      "torch.Size([32])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[False, False, False, False, False, False, False, False, False, False,\n         False, False, False, False, False, False, False, False, False, False,\n         False, False, False,  True,  True,  True,  True,  True,  True,  True,\n          True,  True],\n        [False, False, False, False, False, False, False, False, False, False,\n         False, False, False, False, False, False, False, False, False, False,\n         False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n          True,  True],\n        [False, False, False, False, False, False, False, False, False, False,\n         False, False, False, False, False, False, False, False, False, False,\n          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n          True,  True],\n        [False, False, False, False, False, False, False, False, False, False,\n         False, False, False, False, False, False, False, False, False, False,\n         False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n          True,  True],\n        [False, False, False, False, False, False, False, False, False,  True,\n          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n          True,  True],\n        [False, False, False, False, False, False, False, False, False, False,\n         False, False, False, False, False, False, False, False, False, False,\n         False, False, False, False, False, False, False, False, False, False,\n         False, False]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_list=[]\n",
    "for sample in pad_sequence(data, max_len=None).transpose(1,0):\n",
    "    print(sample.shape)  # [seq_len,batch_size]\n",
    "    padding_mask = (sample == sentence_class.PAD_IDX)\n",
    "    pad_list.append(padding_mask)\n",
    "mask = torch.stack(pad_list, dim=0)\n",
    "mask"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Embedding\n",
    "### TokenEmbedding\n",
    "This cell validates the **TokenEmbedding** layer, which is the first step in BERT’s embedding pipeline. Key points:\n",
    "\n",
    "1. **Embedding Matrix**: BERT’s token embedding is a learnable matrix of shape `[src_len, hidden_size]`,where hidden_size is 768 here. Each row corresponds to a token’s embedding vector, initialized randomly and refined through pre-training.\n",
    "2. **Subword Inputs**: Inputs to this layer are token IDs generated by WordPiece tokenization, where rare or unknown words are split into subword units or mapped to `[UNK]`.\n",
    "3. **Lookup Operation**: Given `src` tensor of shape `[src_len, batch_size]`, the layer performs a lookup for each ID, producing a dense representation of shape `[src_len, batch_size, hidden_size]`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** --------- test TokenEmbedding ------------\n",
      "input_token shape [src_len,batch_size]:  torch.Size([32, 6])\n",
      "input_token embedding shape [src_len,batch_size,hidden_size]: torch.Size([32, 6, 768])\n",
      "\n",
      "tensor([[[ 0.0075,  0.0029, -0.0010,  ...,  0.0143, -0.0421, -0.0066],\n",
      "         [ 0.0075,  0.0029, -0.0010,  ...,  0.0143, -0.0421, -0.0066],\n",
      "         [ 0.0075,  0.0029, -0.0010,  ...,  0.0143, -0.0421, -0.0066],\n",
      "         [ 0.0075,  0.0029, -0.0010,  ...,  0.0143, -0.0421, -0.0066],\n",
      "         [ 0.0075,  0.0029, -0.0010,  ...,  0.0143, -0.0421, -0.0066],\n",
      "         [ 0.0075,  0.0029, -0.0010,  ...,  0.0143, -0.0421, -0.0066]],\n",
      "\n",
      "        [[-0.0256, -0.0010, -0.0138,  ...,  0.0517, -0.0053, -0.0330],\n",
      "         [-0.0130,  0.0355, -0.0006,  ...,  0.0020,  0.0105, -0.0442],\n",
      "         [ 0.0239, -0.0121,  0.0064,  ..., -0.0078,  0.0319,  0.0045],\n",
      "         [ 0.0158,  0.0094,  0.0117,  ...,  0.0343, -0.0256, -0.0114],\n",
      "         [ 0.0127, -0.0028,  0.0413,  ..., -0.0143, -0.0518, -0.0062],\n",
      "         [-0.0022, -0.0432,  0.0027,  ..., -0.0045, -0.0300, -0.0020]],\n",
      "\n",
      "        [[ 0.0223,  0.0361,  0.0264,  ...,  0.0441,  0.0062,  0.0414],\n",
      "         [ 0.0038, -0.0139,  0.0033,  ..., -0.0124, -0.0043, -0.0046],\n",
      "         [ 0.0332, -0.0055,  0.0054,  ..., -0.0083,  0.0243,  0.0283],\n",
      "         [-0.0070, -0.0261, -0.0144,  ..., -0.0262,  0.0050,  0.0062],\n",
      "         [-0.0036, -0.0302,  0.0170,  ...,  0.0070, -0.0150, -0.0002],\n",
      "         [ 0.0149, -0.0182,  0.0167,  ...,  0.0287, -0.0113,  0.0051]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0140,  0.0325, -0.0198,  ..., -0.0145, -0.0050, -0.0020],\n",
      "         [ 0.0140,  0.0325, -0.0198,  ..., -0.0145, -0.0050, -0.0020],\n",
      "         [ 0.0140,  0.0325, -0.0198,  ..., -0.0145, -0.0050, -0.0020],\n",
      "         [ 0.0140,  0.0325, -0.0198,  ..., -0.0145, -0.0050, -0.0020],\n",
      "         [ 0.0140,  0.0325, -0.0198,  ..., -0.0145, -0.0050, -0.0020],\n",
      "         [-0.0072,  0.0030, -0.0072,  ..., -0.0283,  0.0251,  0.0220]],\n",
      "\n",
      "        [[ 0.0140,  0.0325, -0.0198,  ..., -0.0145, -0.0050, -0.0020],\n",
      "         [ 0.0140,  0.0325, -0.0198,  ..., -0.0145, -0.0050, -0.0020],\n",
      "         [ 0.0140,  0.0325, -0.0198,  ..., -0.0145, -0.0050, -0.0020],\n",
      "         [ 0.0140,  0.0325, -0.0198,  ..., -0.0145, -0.0050, -0.0020],\n",
      "         [ 0.0140,  0.0325, -0.0198,  ..., -0.0145, -0.0050, -0.0020],\n",
      "         [-0.0029, -0.0010, -0.0110,  ...,  0.0187, -0.0022, -0.0002]],\n",
      "\n",
      "        [[ 0.0140,  0.0325, -0.0198,  ..., -0.0145, -0.0050, -0.0020],\n",
      "         [ 0.0140,  0.0325, -0.0198,  ..., -0.0145, -0.0050, -0.0020],\n",
      "         [ 0.0140,  0.0325, -0.0198,  ..., -0.0145, -0.0050, -0.0020],\n",
      "         [ 0.0140,  0.0325, -0.0198,  ..., -0.0145, -0.0050, -0.0020],\n",
      "         [ 0.0140,  0.0325, -0.0198,  ..., -0.0145, -0.0050, -0.0020],\n",
      "         [ 0.0031, -0.0204,  0.0167,  ..., -0.0118, -0.0162,  0.0309]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from model import TokenEmbedding\n",
    "\n",
    "json_file = 'bert_base_chinese/config.json'\n",
    "config = BertConfig.from_json_file(json_file)\n",
    "src = pad\n",
    "src = src.transpose(0, 1)  # [src_len, batch_size]\n",
    "\n",
    "token_embedding = TokenEmbedding(vocab_size=config.vocab_size, hidden_size=config.hidden_size)\n",
    "t_embedding = token_embedding(input_ids=src)\n",
    "print(\"***** --------- test TokenEmbedding ------------\")\n",
    "print(\"input_token shape [src_len,batch_size]: \", src.shape)\n",
    "print(f\"input_token embedding shape [src_len,batch_size,hidden_size]: {t_embedding.shape}\\n\")\n",
    "print(t_embedding)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Positional Embedding\n",
    "\n",
    "BERT uses **learned positional embeddings** to encode token order, differing from fixed sinusoidal methods. The core concepts are:\n",
    "\n",
    "1. **Embedding Matrix**\n",
    "   - Shape: `[max_position_embeddings, hidden_size]`.\n",
    "   - Rows correspond to position indices from `0` to `max_position_embeddings-1`, each initialized randomly and fine-tuned during pre-training.\n",
    "\n",
    "2. **Position ID Tensor**\n",
    "   - Generated by `torch.arange(src_len).unsqueeze(0)`, producing a tensor of shape `[1, src_len]`.\n",
    "   - Assigns each token an absolute position index for lookup.\n",
    "\n",
    "3. **Lookup Operation**\n",
    "   - Feeding `position_ids` into the `PositionalEmbedding` layer returns a tensor of shape `[src_len, 1, hidden_size]`.\n",
    "   - Each vector aligns with its corresponding token across the sequence."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]])\n",
      "***** --------- test PositionalEmbedding ------------\n",
      "position_ids shape [1,src_len]:  torch.Size([1, 32])\n",
      "pos embedding shape [src_len, 1, hidden_size]: torch.Size([32, 1, 8])\n",
      "\n",
      "tensor([[[ 3.5126e-02,  1.9238e-02, -9.9565e-03, -1.0535e-02,  1.3769e-02,\n",
      "           5.3469e-03,  2.4950e-03,  1.0606e-02]],\n",
      "\n",
      "        [[ 4.8951e-03,  2.1725e-02,  3.0443e-02, -6.9817e-03, -2.5306e-02,\n",
      "           2.9762e-03, -2.7192e-02,  1.2896e-02]],\n",
      "\n",
      "        [[-3.0778e-02, -1.6687e-02,  2.6079e-03,  1.5348e-02, -2.4240e-02,\n",
      "           8.0362e-03,  4.1460e-02, -7.2073e-03]],\n",
      "\n",
      "        [[ 1.0647e-03,  2.9670e-02, -2.1668e-02, -2.0692e-04, -4.9818e-03,\n",
      "          -1.6159e-03, -4.8928e-03, -1.6830e-02]],\n",
      "\n",
      "        [[-2.2703e-02, -5.7539e-03, -1.1352e-02, -2.1815e-02,  2.0396e-03,\n",
      "           1.6219e-02, -1.2897e-02,  8.0958e-03]],\n",
      "\n",
      "        [[-5.4971e-03,  9.7179e-03,  1.4220e-02, -4.0771e-03,  1.4409e-02,\n",
      "           3.0129e-02,  7.4437e-03, -2.5653e-03]],\n",
      "\n",
      "        [[ 1.2888e-02,  1.6983e-02,  2.5701e-02,  3.0001e-02, -2.8945e-03,\n",
      "          -9.2024e-03, -1.1538e-02, -3.2562e-02]],\n",
      "\n",
      "        [[-1.8033e-02,  2.3912e-02,  1.3145e-02, -1.6665e-02,  1.8668e-02,\n",
      "           2.2848e-02, -1.9966e-02, -1.9988e-02]],\n",
      "\n",
      "        [[ 1.9033e-03, -7.3605e-03, -1.1175e-02,  6.2895e-03, -4.3795e-03,\n",
      "           2.1275e-02,  4.5297e-02,  1.5648e-02]],\n",
      "\n",
      "        [[-1.9161e-02, -1.5149e-02, -4.1399e-02, -2.4939e-03, -5.9933e-03,\n",
      "           5.8676e-03, -2.7736e-03,  9.2717e-03]],\n",
      "\n",
      "        [[ 1.3519e-02, -5.8236e-03, -2.1252e-02,  5.4393e-02, -5.1807e-02,\n",
      "          -2.0293e-04,  1.9671e-02,  9.0303e-03]],\n",
      "\n",
      "        [[ 3.0491e-03, -4.4486e-03, -3.9734e-02,  2.3056e-02, -4.4095e-02,\n",
      "          -1.7046e-02,  7.8029e-03, -1.1635e-02]],\n",
      "\n",
      "        [[-3.1177e-02,  5.6119e-03,  1.1947e-02, -6.2781e-03, -4.9395e-03,\n",
      "          -6.5436e-03,  6.9911e-03,  1.3046e-02]],\n",
      "\n",
      "        [[ 2.8196e-02, -1.2445e-03, -3.2694e-02, -2.6951e-02, -1.2627e-02,\n",
      "          -2.7913e-02,  5.9772e-02, -2.0630e-04]],\n",
      "\n",
      "        [[-5.2678e-03, -2.5414e-02,  1.4836e-03, -2.2075e-03,  4.1876e-03,\n",
      "           1.2480e-02, -9.3796e-03,  1.6715e-02]],\n",
      "\n",
      "        [[-4.2042e-02,  4.1551e-02, -1.9451e-02,  4.8560e-02, -2.2626e-02,\n",
      "           3.2058e-03,  1.1695e-02, -1.0529e-02]],\n",
      "\n",
      "        [[ 3.2698e-02, -6.4874e-03,  1.1376e-02, -1.2120e-03,  9.3761e-03,\n",
      "           2.3436e-04,  2.5236e-03, -1.9771e-04]],\n",
      "\n",
      "        [[ 1.7622e-04, -5.6804e-03, -3.3957e-02,  5.7070e-03, -7.3386e-03,\n",
      "           4.4874e-05, -1.2305e-03,  1.4559e-02]],\n",
      "\n",
      "        [[ 6.4189e-03,  1.6164e-02, -9.5714e-03,  2.2463e-02,  1.6064e-02,\n",
      "           1.3262e-03, -7.2254e-03, -2.3392e-02]],\n",
      "\n",
      "        [[ 2.0664e-02,  2.4776e-02,  3.0503e-02, -6.3070e-04, -9.6004e-03,\n",
      "           2.7706e-02, -2.8236e-02,  2.2821e-02]],\n",
      "\n",
      "        [[ 1.7154e-03,  2.0691e-02,  1.2710e-02,  2.1179e-02,  1.0483e-02,\n",
      "          -1.5059e-02,  4.3349e-03,  1.3519e-02]],\n",
      "\n",
      "        [[ 5.0309e-03,  1.5709e-03,  2.3796e-02,  1.5259e-02, -1.5218e-02,\n",
      "          -1.9446e-02, -1.4911e-03, -9.3799e-03]],\n",
      "\n",
      "        [[ 2.2120e-02, -6.8397e-04,  7.6646e-03,  3.0657e-02, -3.1818e-02,\n",
      "           8.4117e-03, -2.6077e-02,  5.8775e-03]],\n",
      "\n",
      "        [[ 1.3441e-02,  2.3509e-03,  5.7892e-03, -1.5764e-02,  3.1909e-02,\n",
      "           1.2445e-03,  1.3954e-02,  2.9716e-02]],\n",
      "\n",
      "        [[ 3.0885e-03,  3.5983e-03,  2.4468e-02,  3.2209e-02, -4.6146e-03,\n",
      "           9.0321e-03,  1.0416e-03,  7.1956e-03]],\n",
      "\n",
      "        [[ 4.9341e-02, -2.8613e-02, -1.5048e-02,  3.5237e-03, -1.5519e-02,\n",
      "           2.7360e-03, -2.0418e-02, -5.0417e-03]],\n",
      "\n",
      "        [[-1.2130e-02, -1.2077e-03, -3.9224e-02, -5.4412e-03,  1.1008e-02,\n",
      "          -1.3222e-02, -3.3158e-02, -1.0609e-02]],\n",
      "\n",
      "        [[ 5.5475e-03, -2.0090e-02, -2.6179e-02, -5.0118e-03,  2.0097e-02,\n",
      "          -1.4261e-02,  1.1996e-02, -1.7477e-02]],\n",
      "\n",
      "        [[-5.6271e-03, -2.9824e-03,  3.0139e-03, -3.4355e-02,  3.3269e-02,\n",
      "           1.0605e-02, -2.6089e-03,  2.6378e-03]],\n",
      "\n",
      "        [[-3.5142e-02, -1.5104e-02, -1.1774e-03, -3.0229e-03,  1.4262e-02,\n",
      "          -1.6411e-02,  8.6445e-03, -1.9439e-02]],\n",
      "\n",
      "        [[ 3.6694e-03,  1.1588e-02, -1.1128e-02, -1.8046e-02,  3.4490e-03,\n",
      "           1.2907e-02, -6.1143e-03,  4.0123e-02]],\n",
      "\n",
      "        [[-1.7283e-02,  2.5964e-02, -1.0067e-03,  3.9835e-02, -1.6418e-02,\n",
      "           3.5623e-02,  2.5084e-03,  1.3715e-02]]],\n",
      "       grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from model import PositionalEmbedding\n",
    "\n",
    "position_ids = torch.arange(pad[0].size()[0]).expand((1, -1))\n",
    "print(position_ids)\n",
    "pos_embedding = PositionalEmbedding(max_position_embeddings=32,\n",
    "                                        hidden_size=8)\n",
    "p_embedding = pos_embedding(position_ids=position_ids)\n",
    "    # print(pos_embedding.embedding.weight)  # embedding 矩阵\n",
    "    # print(p_embedding)  # positional embedding 结果,\n",
    "print(\"***** --------- test PositionalEmbedding ------------\")\n",
    "print(\"position_ids shape [1,src_len]: \", position_ids.shape)\n",
    "print(f\"pos embedding shape [src_len, 1, hidden_size]: {p_embedding.shape}\\n\")\n",
    "print(p_embedding)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Embeddings Intergration\n",
    "The `BertEmbeddings` class integrates three learnable embedding modules to produce the input representations for BERT’s Transformer encoder:\n",
    "\n",
    "1. **Token Embedding**\n",
    "   - Learns a mapping from token IDs to dense vectors of shape `[src_len, hidden_size]`.\n",
    "   - Captures semantic information of individual subword tokens from pre-training.\n",
    "\n",
    "2. **Positional Embedding**\n",
    "   - A learnable matrix of shape `[src_len, hidden_size]`.\n",
    "   - Encodes absolute token positions, enabling the model to distinguish order without recurrence.\n",
    "\n",
    "3. **Segment (Token Type) Embedding**\n",
    "   - Differentiates sentence segments (e.g., Sentence A vs. Sentence B) in tasks requiring pair inputs.\n",
    "   - Due to the single sentence task,I set this to all 0.\n",
    "\n",
    "**Embedding Fusion**\n",
    "- For each token position `i` in a batch, the final embedding is computed as:\n",
    "  ```text\n",
    "  E[i] = TokenEmb(src_id[i])\n",
    "       + PosEmb(position_id[i])\n",
    "       + SegEmb(token_type_id[i])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape [src_len,batch_size]:  torch.Size([32, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[[ 0.4861, -1.6001, -0.5305,  ..., -1.5493,  0.3996,  0.6168],\n         [ 0.0000, -1.6001, -0.5305,  ..., -1.5493,  0.0000,  0.0000],\n         [ 0.4861, -1.6001, -0.5305,  ..., -1.5493,  0.3996,  0.6168],\n         [ 0.4861, -1.6001, -0.5305,  ..., -0.0000,  0.3996,  0.6168],\n         [ 0.4861, -1.6001, -0.5305,  ..., -1.5493,  0.3996,  0.6168],\n         [ 0.4861, -1.6001, -0.5305,  ..., -1.5493,  0.3996,  0.6168]],\n\n        [[ 0.0363,  0.5027, -0.4244,  ...,  0.8577,  1.0641,  0.1094],\n         [-0.4226, -1.8004,  0.2319,  ...,  0.2035,  0.5910, -0.3899],\n         [-0.3888, -1.3964,  0.2576,  ...,  1.4695,  2.3333,  0.3745],\n         [ 0.8243, -0.9381,  0.7733,  ...,  0.6079,  1.6117,  0.1390],\n         [ 0.9745, -0.1452,  1.0488,  ..., -0.3485,  3.7819,  0.0000],\n         [ 0.9964,  0.1232,  1.9226,  ...,  0.2989,  1.4622, -0.0000]],\n\n        [[ 0.7146, -0.2857, -1.1186,  ..., -0.5450,  1.4823, -0.8822],\n         [ 0.3286, -1.5011, -0.8225,  ...,  0.0000,  1.6215,  1.0085],\n         [ 0.0000,  0.2682, -1.1176,  ...,  1.1118,  0.5942, -0.2732],\n         [ 1.7991, -0.4233,  0.2440,  ..., -0.0000,  1.6510,  0.2355],\n         [ 0.0000, -0.1373,  0.0661,  ...,  0.7418,  1.3998,  0.0000],\n         [ 1.6809, -0.5048,  0.0000,  ...,  0.7639,  1.6203,  0.2849]],\n\n        ...,\n\n        [[ 0.0000, -0.2235,  1.9680,  ..., -1.4349,  1.7616, -0.0000],\n         [ 1.3434, -0.2235,  1.9680,  ..., -1.4349,  1.7616, -0.6852],\n         [ 1.3434, -0.0000,  1.9680,  ..., -1.4349,  1.7616, -0.6852],\n         [ 1.3434, -0.2235,  1.9680,  ..., -1.4349,  1.7616, -0.6852],\n         [ 1.3434, -0.0000,  0.0000,  ..., -0.0000,  1.7616, -0.6852],\n         [ 1.0353, -0.0000,  1.6002,  ...,  0.4476,  1.5881, -1.4686]],\n\n        [[ 1.5422,  0.1255,  0.0000,  ..., -0.9575,  0.6173, -0.1896],\n         [ 1.5422,  0.1255,  0.9559,  ..., -0.9575,  0.6173, -0.1896],\n         [ 1.5422,  0.1255,  0.9559,  ..., -0.9575,  0.6173, -0.1896],\n         [ 1.5422,  0.1255,  0.9559,  ..., -0.9575,  0.6173, -0.1896],\n         [ 1.5422,  0.1255,  0.9559,  ..., -0.9575,  0.6173, -0.1896],\n         [ 1.7711,  0.3708, -0.0000,  ..., -0.0000,  1.4270, -0.2628]],\n\n        [[ 1.6952,  0.8776,  1.3840,  ..., -1.5326,  0.8115,  0.1846],\n         [ 1.6952,  0.8776,  1.3840,  ..., -1.5326,  0.8115,  0.0000],\n         [ 1.6952,  0.8776,  1.3840,  ..., -1.5326,  0.8115,  0.1846],\n         [ 1.6952,  0.8776,  1.3840,  ..., -1.5326,  0.8115,  0.0000],\n         [ 1.6952,  0.8776,  0.0000,  ..., -1.5326,  0.8115,  0.1846],\n         [ 1.2347,  0.1377,  0.0069,  ...,  1.1443,  1.6850,  0.6173]]],\n       grad_fn=<MulBackward0>)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_file = 'bert_base_chinese/config.json'\n",
    "config = BertConfig.from_json_file(json_file)\n",
    "config.__dict__['use_torch_multi_head'] = True\n",
    "config.max_position_embeddings = 518\n",
    "src = pad\n",
    "src = src.transpose(0, 1)  # [src_len, batch_size]\n",
    "print(f\"input shape [src_len,batch_size]: \", src.shape)\n",
    "token_type_ids = torch.where(pad == 0, torch.zeros_like(pad), torch.zeros_like(pad)).transpose(0,1)\n",
    "attention_mask = mask\n",
    "#\n",
    "# # ------ BertEmbedding -------\n",
    "bert_embedding = BertEmbeddings(config)\n",
    "bert_embedding_result = bert_embedding(src, token_type_ids=token_type_ids)\n",
    "bert_embedding_result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BERT Components\n",
    "This section validates BERT’s core Transformer blocks by testing self-attention, single-layer, and full encoder stack. Key BERT-specific insights:\n",
    "\n",
    "1. **Multi-Head Self-Attention (`BertAttention`)**\n",
    "   - **Query/Key/Value Projections**: Inputs of shape `[src_len, batch_size, hidden_size]` are linearly projected into Q, K, V tensors for each of the `num_attention_heads` heads.\n",
    "   - **Scaled Dot-Product**: Each head computes attention weights via `softmax((Q·K^T) / sqrt(d_k))`, focusing on context tokens.\n",
    "   - **Head Concatenation**: Outputs from all heads are concatenated and passed through a final linear layer, then added back to the input via a residual connection.\n",
    "   - **LayerNorm & Dropout**: Normalizes post-attention sums and applies dropout for regularization.\n",
    "\n",
    "2. **Transformer Block (`BertLayer`)**\n",
    "   - **Attention Sub-Layer**: As above, produces contextualized embeddings.\n",
    "   - **Feed-Forward Sub-Layer**: Two-layer MLP (`BertIntermediate` + `BertOutput`) with activation (GELU) projects to `intermediate_size` and back to `hidden_size`.\n",
    "   - **Residual & LayerNorm**: Each sub-layer uses a residual connection followed by LayerNorm to preserve gradients and stabilize training.\n",
    "\n",
    "3. **Encoder Stack (`BertEncoder`)**\n",
    "   - **Layer Stacking**: BERT stacks `num_hidden_layers` identical `BertLayer` modules (e.g., 12 layers in base configuration).\n",
    "   - **Hidden States Output**: Returns outputs of each layer, enabling introspection of hidden representations at various depths.\n",
    "\n",
    "**Practical Implications**\n",
    "- Multi-head attention enables BERT to jointly attend to information from different representation subspaces at different positions.\n",
    "- Deep stacking captures hierarchical linguistic features: lower layers model local syntax, higher layers abstract semantics.\n",
    "- Residual connections and LayerNorm ensure stable gradient flow and effective pre-training over long sequences."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertAttention output shape [src_len, batch_size, hidden_size]:  torch.Size([32, 6, 768])\n",
      "num of BertEncoder [config.num_hidden_layers]:  12\n",
      "each output shape in BertEncoder [src_len, batch_size, hidden_size]:  torch.Size([32, 6, 768])\n",
      "[tensor([[[ 4.7121e-01, -1.3970e+00, -6.5509e-01,  ..., -1.3581e+00,\n",
      "           1.1114e-01,  8.1716e-01],\n",
      "         [ 5.8348e-02, -1.2759e+00, -4.4585e-01,  ..., -1.2719e+00,\n",
      "          -2.0997e-01,  1.4221e-01],\n",
      "         [ 2.2763e-01, -1.2297e+00, -5.5650e-01,  ..., -1.1705e+00,\n",
      "           1.6773e-01,  6.3508e-01],\n",
      "         [ 5.2786e-02, -1.3453e+00, -6.5582e-01,  ...,  3.0606e-01,\n",
      "           1.3006e-01,  7.3074e-01],\n",
      "         [ 4.6505e-01, -9.9038e-01, -4.2832e-01,  ..., -1.4094e+00,\n",
      "           7.8336e-02,  7.2582e-01],\n",
      "         [ 2.5764e-01, -1.3428e+00, -6.3295e-01,  ..., -1.0395e+00,\n",
      "          -1.0489e-02,  3.8393e-01]],\n",
      "\n",
      "        [[-2.0534e-01,  4.5511e-01, -6.9092e-02,  ...,  1.1451e+00,\n",
      "           8.4206e-01,  4.7748e-01],\n",
      "         [-7.5937e-01, -1.6854e+00,  4.2461e-01,  ...,  6.3935e-01,\n",
      "           3.8399e-01, -1.9370e-01],\n",
      "         [-6.1883e-01, -1.1258e+00,  7.4460e-01,  ...,  1.1022e+00,\n",
      "           1.6127e+00,  6.3720e-01],\n",
      "         [ 6.0398e-01, -4.9983e-01,  1.0081e+00,  ...,  1.1666e+00,\n",
      "           1.2060e+00,  5.1493e-01],\n",
      "         [ 3.2476e-01,  2.0809e-01,  1.5707e+00,  ...,  5.8292e-02,\n",
      "           3.2304e+00,  1.7507e-01],\n",
      "         [ 5.5884e-01,  1.7961e-01,  2.1568e+00,  ...,  8.3217e-01,\n",
      "           1.1206e+00,  4.4476e-01]],\n",
      "\n",
      "        [[ 8.4742e-01,  8.8947e-02, -1.0513e+00,  ..., -1.5678e-01,\n",
      "           1.4618e+00, -4.9869e-01],\n",
      "         [ 5.0984e-01, -1.2105e+00, -5.7862e-01,  ...,  2.4634e-01,\n",
      "           1.1441e+00,  8.7652e-01],\n",
      "         [-4.5596e-02,  6.6060e-01, -7.6498e-01,  ...,  8.5239e-01,\n",
      "           1.0209e-01, -3.8631e-01],\n",
      "         [ 1.3843e+00, -2.0872e-01,  3.2966e-01,  ..., -4.2748e-02,\n",
      "           1.2027e+00,  1.5117e-01],\n",
      "         [-2.4076e-01,  3.4241e-01, -6.2016e-03,  ...,  6.3191e-01,\n",
      "           7.0793e-01, -3.0442e-02],\n",
      "         [ 9.9706e-01, -2.0868e-01, -2.7924e-01,  ...,  7.2866e-01,\n",
      "           1.2787e+00,  2.5834e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.6845e-01,  9.4610e-02,  1.9882e+00,  ..., -1.0751e+00,\n",
      "           1.1181e+00,  1.4937e-01],\n",
      "         [ 1.0790e+00,  2.8419e-01,  1.9549e+00,  ..., -1.2537e+00,\n",
      "           1.1371e+00, -6.6836e-01],\n",
      "         [ 1.1614e+00,  3.5154e-02,  1.9744e+00,  ..., -1.1824e+00,\n",
      "           1.5107e+00, -8.4532e-01],\n",
      "         [ 1.0747e+00, -8.0644e-02,  2.2532e+00,  ..., -1.1753e+00,\n",
      "           8.3711e-01, -5.3924e-01],\n",
      "         [ 9.3485e-01,  5.0391e-01,  5.8138e-01,  ..., -2.6424e-01,\n",
      "           9.5661e-01, -5.4752e-01],\n",
      "         [ 7.9866e-01,  8.7541e-02,  1.9124e+00,  ...,  5.3947e-01,\n",
      "           8.2379e-01, -1.2720e+00]],\n",
      "\n",
      "        [[ 9.9697e-01,  1.7783e-01,  2.3405e-01,  ..., -6.3566e-01,\n",
      "           2.4394e-01, -1.7964e-01],\n",
      "         [ 1.0587e+00,  3.2886e-01,  1.1885e+00,  ..., -7.2944e-01,\n",
      "           2.8029e-01, -9.1962e-02],\n",
      "         [ 1.0287e+00,  2.7013e-01,  1.2856e+00,  ..., -5.7694e-01,\n",
      "           3.8699e-02, -1.0863e-01],\n",
      "         [ 1.0177e+00,  4.0535e-01,  1.2344e+00,  ..., -5.7744e-01,\n",
      "           1.9503e-01, -4.2789e-04],\n",
      "         [ 9.6611e-01,  2.1470e-01,  1.2361e+00,  ..., -6.8650e-01,\n",
      "           9.9295e-02, -5.4958e-01],\n",
      "         [ 1.8395e+00,  3.4987e-01,  2.4314e-01,  ...,  8.8125e-02,\n",
      "           9.8814e-01, -3.1277e-01]],\n",
      "\n",
      "        [[ 1.5610e+00,  8.6759e-01,  1.3508e+00,  ..., -1.3516e+00,\n",
      "           5.0263e-01,  4.7804e-01],\n",
      "         [ 1.6154e+00,  9.9976e-01,  1.4304e+00,  ..., -1.5520e+00,\n",
      "           2.1350e-01, -3.9838e-01],\n",
      "         [ 1.1639e+00,  1.0553e+00,  1.4972e+00,  ..., -1.2336e+00,\n",
      "           5.9267e-01,  1.6416e-01],\n",
      "         [ 1.3635e+00,  1.1578e+00,  1.4561e+00,  ..., -1.3081e+00,\n",
      "           3.7394e-01, -4.3401e-02],\n",
      "         [ 1.2333e+00,  1.0831e+00,  2.7225e-01,  ..., -1.3126e+00,\n",
      "           4.9645e-01,  3.7148e-01],\n",
      "         [ 7.3979e-01,  2.4356e-01,  7.2547e-02,  ...,  1.1967e+00,\n",
      "           1.2976e+00,  7.9077e-01]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 4.7114e-01, -1.2964e+00, -1.2747e+00,  ..., -1.1330e+00,\n",
      "           1.5170e-01,  1.0504e+00],\n",
      "         [ 3.0236e-01, -1.3392e+00, -1.0897e+00,  ..., -1.3146e+00,\n",
      "          -5.4710e-01,  3.2798e-01],\n",
      "         [ 4.3913e-01, -1.1277e+00, -1.0081e+00,  ..., -1.1806e+00,\n",
      "          -2.2964e-01,  1.0671e+00],\n",
      "         [ 1.4651e-01, -1.2667e+00, -1.0830e+00,  ...,  1.6748e-01,\n",
      "          -3.5791e-01,  9.1264e-01],\n",
      "         [ 3.9900e-01, -9.2659e-01, -3.9762e-01,  ..., -1.7943e+00,\n",
      "          -4.1848e-01,  7.4404e-01],\n",
      "         [ 3.1176e-01, -1.2313e+00, -9.8562e-01,  ..., -8.5190e-01,\n",
      "          -3.2556e-01,  5.6074e-01]],\n",
      "\n",
      "        [[ 1.2515e-01,  1.6322e-01, -1.9651e-01,  ...,  1.0735e+00,\n",
      "           1.1358e+00,  4.6444e-01],\n",
      "         [-4.5255e-01, -1.7235e+00,  8.1252e-02,  ...,  7.1894e-01,\n",
      "           3.6341e-01, -1.2187e-01],\n",
      "         [-2.7880e-01, -1.1737e+00,  3.7347e-01,  ...,  1.0527e+00,\n",
      "           1.6058e+00,  9.6534e-01],\n",
      "         [ 8.2082e-01, -7.7965e-01,  5.2506e-01,  ...,  7.6909e-01,\n",
      "           1.0753e+00,  2.7742e-01],\n",
      "         [ 5.6544e-01,  2.2439e-02,  1.0066e+00,  ..., -3.0366e-01,\n",
      "           3.1990e+00,  1.2492e-01],\n",
      "         [ 6.2597e-01,  1.0354e-01,  1.4359e+00,  ...,  7.2721e-01,\n",
      "           1.4921e+00,  6.9644e-01]],\n",
      "\n",
      "        [[ 9.8202e-01, -1.3509e-01, -1.0802e+00,  ..., -2.1487e-01,\n",
      "           1.2248e+00, -2.3889e-01],\n",
      "         [ 4.6423e-01, -1.1614e+00, -7.7423e-01,  ...,  3.3943e-01,\n",
      "           1.2776e+00,  1.2131e+00],\n",
      "         [ 2.4194e-01,  7.7930e-01, -7.8021e-01,  ...,  9.6067e-01,\n",
      "          -1.2827e-02, -4.6418e-01],\n",
      "         [ 1.0588e+00, -4.1052e-01,  2.5915e-02,  ..., -7.9879e-02,\n",
      "           1.0866e+00,  3.9424e-01],\n",
      "         [-3.4194e-01,  1.7452e-01, -2.1109e-01,  ...,  3.0102e-01,\n",
      "           7.1166e-01,  2.5311e-02],\n",
      "         [ 1.1451e+00, -2.7117e-01, -7.5397e-01,  ...,  6.1979e-01,\n",
      "           9.5890e-01,  2.1598e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-6.9067e-02, -2.4569e-01,  1.2831e+00,  ..., -9.5665e-01,\n",
      "           6.8946e-01,  2.6758e-01],\n",
      "         [ 1.1912e+00,  2.5995e-01,  1.5045e+00,  ..., -1.0697e+00,\n",
      "           6.9868e-01, -5.3081e-01],\n",
      "         [ 1.2488e+00, -4.5022e-02,  1.4275e+00,  ..., -1.1949e+00,\n",
      "           9.2996e-01, -7.5069e-01],\n",
      "         [ 1.1648e+00, -3.5989e-01,  1.8276e+00,  ..., -1.3354e+00,\n",
      "           6.1242e-01, -1.4535e-01],\n",
      "         [ 1.1370e+00,  4.4590e-01,  1.2440e-01,  ..., -2.0967e-01,\n",
      "           5.2956e-01, -4.4815e-01],\n",
      "         [ 1.1205e+00, -3.1022e-01,  1.7227e+00,  ...,  5.4822e-01,\n",
      "           5.3090e-01, -1.0187e+00]],\n",
      "\n",
      "        [[ 9.0528e-01,  5.8119e-02, -2.0612e-01,  ..., -4.8231e-01,\n",
      "          -5.8632e-02,  4.6556e-02],\n",
      "         [ 1.0591e+00,  1.3530e-01,  7.2371e-01,  ..., -6.0030e-01,\n",
      "          -1.0355e-01,  6.5797e-02],\n",
      "         [ 9.2639e-01,  2.0125e-01,  9.8896e-01,  ..., -5.1944e-01,\n",
      "          -2.3642e-01,  2.3682e-01],\n",
      "         [ 5.1767e-01,  2.9756e-01,  7.6678e-01,  ..., -3.5724e-01,\n",
      "           1.7349e-01,  2.0938e-01],\n",
      "         [ 7.0626e-01,  3.9419e-01,  7.3451e-01,  ..., -8.1096e-01,\n",
      "           1.0194e-01, -3.9841e-01],\n",
      "         [ 1.8284e+00,  2.5355e-03, -1.4886e-02,  ..., -2.1722e-03,\n",
      "           8.2183e-01, -1.2837e-01]],\n",
      "\n",
      "        [[ 1.4370e+00,  4.3034e-01,  8.5354e-01,  ..., -1.1511e+00,\n",
      "           5.1015e-01,  5.0218e-01],\n",
      "         [ 1.9485e+00,  6.3505e-01,  8.4541e-01,  ..., -1.3500e+00,\n",
      "           1.6100e-01, -2.3728e-01],\n",
      "         [ 1.1740e+00,  7.9858e-01,  9.0485e-01,  ..., -1.2687e+00,\n",
      "           3.4588e-01,  4.4397e-01],\n",
      "         [ 1.3896e+00,  7.8486e-01,  1.0033e+00,  ..., -1.3304e+00,\n",
      "           1.0639e-01,  4.0738e-02],\n",
      "         [ 1.1979e+00,  6.6550e-01, -2.7682e-02,  ..., -1.5935e+00,\n",
      "           3.2880e-01,  4.2240e-01],\n",
      "         [ 8.8420e-01, -1.8776e-01, -5.6139e-01,  ...,  1.0719e+00,\n",
      "           1.2819e+00,  7.1627e-01]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 9.0905e-01, -8.2018e-01, -1.5626e+00,  ..., -1.1724e+00,\n",
      "           3.7938e-01,  6.8724e-01],\n",
      "         [ 7.7518e-01, -1.2178e+00, -1.3999e+00,  ..., -1.5342e+00,\n",
      "          -2.0518e-01, -2.5913e-01],\n",
      "         [ 9.2558e-01, -7.7390e-01, -1.3907e+00,  ..., -1.2360e+00,\n",
      "          -1.2384e-01,  8.8568e-01],\n",
      "         [ 3.9017e-01, -1.0140e+00, -1.3965e+00,  ...,  9.3881e-02,\n",
      "          -2.5071e-01,  6.6008e-01],\n",
      "         [ 8.2486e-01, -6.9282e-01, -7.0142e-01,  ..., -1.7464e+00,\n",
      "          -1.9204e-01,  6.4940e-01],\n",
      "         [ 6.3678e-01, -6.9576e-01, -1.1994e+00,  ..., -1.0126e+00,\n",
      "          -2.6251e-01,  3.9278e-01]],\n",
      "\n",
      "        [[ 8.4716e-01,  4.8428e-01, -8.6297e-01,  ...,  1.1166e+00,\n",
      "           1.2518e+00,  4.3160e-01],\n",
      "         [ 4.7910e-02, -1.2984e+00, -6.3355e-01,  ...,  6.9658e-01,\n",
      "           8.8906e-01, -5.5694e-01],\n",
      "         [ 3.1434e-01, -9.4358e-01, -2.8414e-01,  ...,  8.2919e-01,\n",
      "           1.6579e+00,  8.8251e-01],\n",
      "         [ 1.3779e+00, -8.0434e-01, -8.4860e-02,  ...,  8.6101e-01,\n",
      "           1.1405e+00,  2.7651e-02],\n",
      "         [ 1.1566e+00,  5.0109e-01,  5.9487e-01,  ..., -4.5786e-01,\n",
      "           3.1104e+00, -7.3438e-02],\n",
      "         [ 1.5287e+00,  3.7512e-01,  8.8355e-01,  ...,  7.1730e-01,\n",
      "           1.6302e+00,  2.3020e-01]],\n",
      "\n",
      "        [[ 1.5127e+00,  7.9562e-02, -1.5792e+00,  ..., -1.3256e-01,\n",
      "           1.1217e+00, -3.8592e-01],\n",
      "         [ 1.0489e+00, -8.2868e-01, -1.0624e+00,  ...,  3.9674e-01,\n",
      "           8.1447e-01,  7.5808e-01],\n",
      "         [ 6.7123e-01,  1.0701e+00, -6.4998e-01,  ...,  7.8335e-01,\n",
      "          -2.5784e-02, -6.8831e-01],\n",
      "         [ 1.4326e+00, -2.0201e-01, -4.8839e-01,  ...,  1.4440e-01,\n",
      "           9.8052e-01,  1.2063e-01],\n",
      "         [-6.3433e-02,  2.6772e-01, -6.0081e-01,  ...,  5.5593e-01,\n",
      "           7.3509e-01, -2.0438e-01],\n",
      "         [ 1.4166e+00,  1.0461e-02, -1.0033e+00,  ...,  8.0785e-01,\n",
      "           9.0828e-01,  1.4665e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 4.7071e-01, -5.4277e-02,  9.2787e-01,  ..., -1.2357e+00,\n",
      "           5.4470e-01,  2.7851e-01],\n",
      "         [ 1.7791e+00,  4.5498e-01,  1.3478e+00,  ..., -1.2465e+00,\n",
      "           5.5292e-01, -7.3420e-01],\n",
      "         [ 1.7799e+00,  2.1328e-01,  1.1621e+00,  ..., -1.4810e+00,\n",
      "           6.0574e-01, -6.6812e-01],\n",
      "         [ 1.5404e+00, -2.3531e-01,  1.2674e+00,  ..., -1.4989e+00,\n",
      "           5.1830e-01, -3.8791e-01],\n",
      "         [ 1.4484e+00,  6.0414e-01, -1.0212e-01,  ..., -3.1488e-01,\n",
      "           3.4245e-01, -6.6981e-01],\n",
      "         [ 1.5115e+00,  5.5366e-02,  1.3395e+00,  ...,  2.0718e-01,\n",
      "           7.2432e-01, -1.0591e+00]],\n",
      "\n",
      "        [[ 1.6575e+00,  4.3288e-01, -5.8676e-01,  ..., -5.1861e-01,\n",
      "          -2.0226e-01, -1.5570e-01],\n",
      "         [ 1.6198e+00,  4.7186e-01,  2.2703e-01,  ..., -6.3604e-01,\n",
      "          -1.5438e-01, -3.1199e-01],\n",
      "         [ 1.1768e+00,  4.9164e-01,  4.6787e-01,  ..., -7.4468e-01,\n",
      "          -5.1787e-01,  1.9148e-02],\n",
      "         [ 1.0095e+00,  5.1264e-01,  3.7548e-01,  ..., -2.8214e-01,\n",
      "          -1.7299e-01, -1.3296e-01],\n",
      "         [ 1.2374e+00,  6.0061e-01,  1.6852e-01,  ..., -7.2001e-01,\n",
      "           1.4875e-01, -7.7357e-01],\n",
      "         [ 2.3188e+00,  4.4480e-01, -5.4800e-01,  ...,  4.6973e-02,\n",
      "           6.3461e-01, -4.0307e-01]],\n",
      "\n",
      "        [[ 1.9372e+00,  7.6141e-01,  3.0695e-01,  ..., -9.4640e-01,\n",
      "           4.3972e-01,  3.3439e-01],\n",
      "         [ 2.3884e+00,  9.5509e-01,  4.9808e-01,  ..., -1.1878e+00,\n",
      "           1.5572e-01, -5.2466e-01],\n",
      "         [ 1.4175e+00,  1.1391e+00,  4.4129e-01,  ..., -9.8658e-01,\n",
      "           2.0817e-03,  1.0177e-01],\n",
      "         [ 1.8776e+00,  9.6809e-01,  5.6446e-01,  ..., -1.1816e+00,\n",
      "           5.6487e-04,  2.1767e-01],\n",
      "         [ 1.6336e+00,  6.1802e-01, -3.9893e-01,  ..., -1.1847e+00,\n",
      "           1.1249e-01, -9.7957e-02],\n",
      "         [ 1.5013e+00,  2.1473e-01, -8.1144e-01,  ...,  7.3594e-01,\n",
      "           1.4324e+00,  1.3954e-01]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.6431, -0.8200, -1.0666,  ..., -0.7909,  0.0608,  0.7184],\n",
      "         [ 0.8269, -1.1813, -1.3947,  ..., -1.3666, -0.3252, -0.1686],\n",
      "         [ 0.9260, -0.8484, -1.4256,  ..., -1.1053, -0.3142,  0.6786],\n",
      "         [ 0.5104, -0.9550, -1.1029,  ...,  0.4382, -0.1585,  0.5308],\n",
      "         [ 0.8795, -0.6973, -0.6996,  ..., -1.3591, -0.3995,  0.4843],\n",
      "         [ 0.7184, -0.6519, -1.3563,  ..., -0.5385, -0.2948,  0.2828]],\n",
      "\n",
      "        [[ 0.8758,  0.3243, -0.4695,  ...,  0.9160,  1.0902,  0.4144],\n",
      "         [ 0.3176, -1.3179, -0.5933,  ...,  0.4458,  0.8019, -0.6091],\n",
      "         [ 0.4625, -0.6500,  0.3463,  ...,  0.9314,  1.3212,  0.8859],\n",
      "         [ 1.5201, -0.9729,  0.0584,  ...,  0.5584,  1.0702,  0.0444],\n",
      "         [ 1.0242,  0.6395,  0.7453,  ..., -0.5809,  2.8419, -0.4244],\n",
      "         [ 1.3718,  0.4102,  0.6366,  ...,  0.4939,  1.3788,  0.1459]],\n",
      "\n",
      "        [[ 1.4700,  0.1731, -1.5003,  ...,  0.0556,  0.5660, -0.3029],\n",
      "         [ 1.1583, -0.6139, -1.2659,  ...,  0.5891,  0.7589,  0.9566],\n",
      "         [ 0.6565,  0.8846, -0.6417,  ...,  0.7798, -0.3452, -0.7852],\n",
      "         [ 1.3349, -0.0986, -0.5075,  ...,  0.1091,  0.5615,  0.4419],\n",
      "         [ 0.1226,  0.4429, -0.5706,  ...,  0.5241,  0.4043, -0.2491],\n",
      "         [ 1.4820,  0.0891, -1.1431,  ...,  1.2123,  0.7767,  0.1949]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.3325, -0.1828,  1.1953,  ..., -1.2922,  0.3819,  0.6837],\n",
      "         [ 1.6116,  0.6324,  1.4604,  ..., -1.4562,  0.4405, -0.3285],\n",
      "         [ 1.6437,  0.2604,  1.3827,  ..., -1.5821,  0.4953, -0.3758],\n",
      "         [ 1.2368, -0.1512,  1.4288,  ..., -1.5129,  0.4140,  0.0715],\n",
      "         [ 1.2155,  0.5374,  0.1638,  ..., -0.7401, -0.0099, -0.2638],\n",
      "         [ 1.2915,  0.0983,  1.6535,  ..., -0.1076,  0.5673, -0.7360]],\n",
      "\n",
      "        [[ 1.4306,  0.3855, -0.4210,  ..., -0.9704, -0.3307,  0.2940],\n",
      "         [ 1.5154,  0.3645,  0.2749,  ..., -1.0243, -0.1766,  0.2898],\n",
      "         [ 1.1309,  0.4439,  0.5126,  ..., -0.9796, -0.6645,  0.0974],\n",
      "         [ 0.7946,  0.4994,  0.1614,  ..., -0.4817, -0.2741,  0.4003],\n",
      "         [ 0.9262,  0.5790,  0.2879,  ..., -0.7849, -0.0465, -0.2834],\n",
      "         [ 2.0660,  0.4465, -0.2742,  ...,  0.3413,  0.6119,  0.0305]],\n",
      "\n",
      "        [[ 1.5770,  0.5929,  0.6166,  ..., -0.9742,  0.2051,  0.6992],\n",
      "         [ 2.1391,  1.0113,  0.6816,  ..., -1.4436, -0.0417, -0.1796],\n",
      "         [ 1.1254,  0.9455,  0.5752,  ..., -1.1354, -0.2137,  0.2233],\n",
      "         [ 1.4630,  0.7203,  0.5269,  ..., -1.3114, -0.0135,  0.3988],\n",
      "         [ 1.2593,  0.7161, -0.1607,  ..., -1.4749,  0.0885, -0.2480],\n",
      "         [ 1.0850,  0.3050, -0.8979,  ...,  0.5083,  1.3811,  0.4692]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.5002, -0.9221, -0.4279,  ..., -1.2674,  0.3842,  0.1501],\n",
      "         [ 0.6295, -1.4343, -0.8999,  ..., -1.9875, -0.2813, -0.5648],\n",
      "         [ 0.7047, -1.1600, -0.8934,  ..., -1.5027, -0.0753,  0.2711],\n",
      "         [ 0.1176, -0.9562, -0.5595,  ..., -0.3054,  0.1619,  0.2791],\n",
      "         [ 0.5867, -0.7821,  0.1984,  ..., -1.6606, -0.3655,  0.2081],\n",
      "         [ 0.3170, -0.5649, -1.3284,  ..., -1.2543, -0.0137, -0.0838]],\n",
      "\n",
      "        [[ 0.6880,  0.3171, -0.3632,  ...,  0.5115,  1.3193, -0.1661],\n",
      "         [ 0.1955, -1.2355,  0.1085,  ...,  0.2314,  0.7796, -1.0971],\n",
      "         [ 0.1297, -0.9518,  0.9300,  ...,  0.5942,  1.2234,  0.3254],\n",
      "         [ 1.0478, -0.7409,  0.1539,  ..., -0.1299,  0.8408, -0.3505],\n",
      "         [ 0.5924,  0.4670,  1.3545,  ..., -0.8932,  2.4134, -0.9295],\n",
      "         [ 0.9537,  0.4254,  1.1929,  ..., -0.1767,  1.4617, -0.2235]],\n",
      "\n",
      "        [[ 1.1604,  0.1800, -1.0308,  ..., -0.2969,  1.0171, -0.7662],\n",
      "         [ 1.2380, -0.7245, -1.2630,  ...,  0.0751,  1.0725,  0.7430],\n",
      "         [ 0.3677,  0.6500, -0.1851,  ...,  0.3887, -0.0684, -0.7620],\n",
      "         [ 0.9969, -0.0481, -0.0203,  ..., -0.2758,  0.7062,  0.1897],\n",
      "         [-0.1357,  0.5807,  0.0142,  ...,  0.2067,  0.5425, -0.2752],\n",
      "         [ 1.1619, -0.0244, -0.3436,  ...,  0.6047,  0.8316, -0.1649]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1830, -0.1275,  0.8927,  ..., -1.6967,  0.6345,  0.1979],\n",
      "         [ 1.5486,  0.7215,  1.7863,  ..., -2.0009,  0.4795, -0.5311],\n",
      "         [ 1.2766,  0.3197,  1.5543,  ..., -1.7271,  0.6712, -0.7077],\n",
      "         [ 0.8896,  0.0403,  1.5585,  ..., -2.0874,  0.7171, -0.3815],\n",
      "         [ 0.9169,  0.6840,  0.7238,  ..., -1.2372,  0.0297, -0.6169],\n",
      "         [ 0.7611,  0.1943,  2.0213,  ..., -0.4311,  0.6282, -0.9361]],\n",
      "\n",
      "        [[ 1.1508,  0.5272, -0.1949,  ..., -1.1275,  0.0344, -0.1035],\n",
      "         [ 1.1296,  0.4002,  0.5261,  ..., -1.5598, -0.0111, -0.1711],\n",
      "         [ 0.8866,  0.4741,  0.2397,  ..., -1.2886, -0.4084, -0.3171],\n",
      "         [ 0.3143,  0.7055,  0.3820,  ..., -0.9103,  0.1270, -0.0804],\n",
      "         [ 0.7487,  0.7985,  0.6926,  ..., -1.2292, -0.1275, -0.7617],\n",
      "         [ 1.9915,  0.6994,  0.1782,  ...,  0.0302,  0.6021, -0.2322]],\n",
      "\n",
      "        [[ 1.3207,  0.5367,  0.9192,  ..., -1.4401,  0.4373,  0.1594],\n",
      "         [ 1.7230,  1.1338,  1.0148,  ..., -1.8213,  0.1283, -0.3782],\n",
      "         [ 0.9211,  0.9554,  0.8324,  ..., -1.5133, -0.0464, -0.1503],\n",
      "         [ 1.0372,  0.8537,  0.9043,  ..., -1.9405,  0.2727, -0.1754],\n",
      "         [ 1.0625,  0.6225,  0.5929,  ..., -2.0781,  0.0702, -0.5962],\n",
      "         [ 0.8538,  0.3199, -0.3702,  ..., -0.3100,  1.7296,  0.1077]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 4.5713e-01, -1.0491e+00,  4.3771e-01,  ..., -1.1660e+00,\n",
      "           2.0513e-01, -9.6284e-02],\n",
      "         [ 3.3383e-01, -1.6790e+00, -3.1582e-01,  ..., -1.7321e+00,\n",
      "          -6.9175e-01, -6.0821e-01],\n",
      "         [ 5.9941e-01, -1.3070e+00, -6.2531e-01,  ..., -1.5153e+00,\n",
      "          -4.4238e-01,  3.8153e-02],\n",
      "         [-3.5543e-02, -8.5927e-01,  3.3760e-01,  ..., -4.3907e-01,\n",
      "          -1.4833e-01, -5.4742e-02],\n",
      "         [ 3.7586e-01, -9.8559e-01,  1.0121e+00,  ..., -1.7838e+00,\n",
      "          -4.3101e-01,  1.4237e-01],\n",
      "         [-3.2754e-03, -7.6334e-01, -5.9936e-01,  ..., -1.1031e+00,\n",
      "          -4.3643e-01, -1.0073e-01]],\n",
      "\n",
      "        [[ 6.4740e-01,  3.1971e-01,  5.8250e-01,  ...,  3.5637e-01,\n",
      "           9.2686e-01, -3.2169e-01],\n",
      "         [ 2.3923e-01, -1.3302e+00,  8.5056e-01,  ...,  1.0023e-01,\n",
      "           5.3911e-02, -8.1073e-01],\n",
      "         [ 4.1011e-02, -1.0114e+00,  1.3904e+00,  ...,  2.3253e-01,\n",
      "           8.1106e-01,  2.0699e-01],\n",
      "         [ 7.4334e-01, -7.4328e-01,  9.3151e-01,  ..., -3.4114e-01,\n",
      "           3.0703e-01, -5.5200e-01],\n",
      "         [ 2.9643e-01,  3.9054e-01,  1.8241e+00,  ..., -1.0135e+00,\n",
      "           2.0385e+00, -9.4705e-01],\n",
      "         [ 9.7768e-01, -6.2041e-02,  1.7346e+00,  ..., -1.9812e-01,\n",
      "           9.0916e-01, -3.2245e-01]],\n",
      "\n",
      "        [[ 1.3692e+00,  1.2013e-01, -3.6398e-01,  ..., -5.7668e-01,\n",
      "           3.0842e-01, -6.8873e-01],\n",
      "         [ 1.3533e+00, -9.8094e-01, -6.1827e-01,  ...,  2.1424e-01,\n",
      "           5.3623e-01,  4.0428e-01],\n",
      "         [ 3.4728e-01,  4.2309e-01,  5.7646e-01,  ...,  4.6881e-02,\n",
      "          -6.2789e-01, -8.1613e-01],\n",
      "         [ 9.2449e-01, -6.2538e-04,  6.8354e-01,  ..., -4.3789e-01,\n",
      "           3.1228e-01,  3.8187e-02],\n",
      "         [-3.4710e-03,  5.2113e-01,  8.4793e-01,  ..., -2.7026e-01,\n",
      "           7.4224e-02, -5.5632e-01],\n",
      "         [ 1.2240e+00, -4.9808e-01, -1.8205e-01,  ...,  4.9303e-01,\n",
      "           4.0538e-01, -2.7704e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-5.5412e-02, -2.0711e-01,  1.4730e+00,  ..., -1.5391e+00,\n",
      "          -1.4708e-02,  1.4443e-01],\n",
      "         [ 1.1571e+00,  7.0491e-01,  1.5936e+00,  ..., -1.9522e+00,\n",
      "           4.5019e-02, -5.6743e-01],\n",
      "         [ 8.2038e-01,  1.6834e-03,  1.6222e+00,  ..., -1.5668e+00,\n",
      "           1.4531e-01, -6.7755e-01],\n",
      "         [ 5.4052e-01,  6.6044e-02,  2.0886e+00,  ..., -2.0867e+00,\n",
      "           4.0733e-01, -6.6644e-01],\n",
      "         [ 5.8822e-01,  6.8031e-01,  1.3233e+00,  ..., -1.5068e+00,\n",
      "          -3.5671e-01, -6.8828e-01],\n",
      "         [ 5.9876e-01, -4.5736e-02,  2.7097e+00,  ..., -6.2786e-01,\n",
      "           4.3998e-01, -9.7200e-01]],\n",
      "\n",
      "        [[ 8.4446e-01,  3.7678e-01,  4.8123e-01,  ..., -9.0424e-01,\n",
      "          -6.4049e-01,  9.1925e-02],\n",
      "         [ 8.6275e-01,  8.4470e-02,  1.0904e+00,  ..., -1.3207e+00,\n",
      "          -7.8452e-01, -1.1245e-01],\n",
      "         [ 6.0256e-01,  9.1496e-02,  1.0010e+00,  ..., -1.0807e+00,\n",
      "          -1.2738e+00, -1.8184e-01],\n",
      "         [ 5.3768e-02,  5.4850e-01,  4.6687e-01,  ..., -7.9976e-01,\n",
      "           4.9205e-02, -2.7344e-02],\n",
      "         [ 3.6353e-01,  5.3773e-01,  1.3615e+00,  ..., -1.4393e+00,\n",
      "          -7.2538e-01, -8.3932e-01],\n",
      "         [ 1.5963e+00,  2.3464e-01,  7.8335e-01,  ...,  4.7850e-02,\n",
      "          -2.2615e-01, -7.0572e-03]],\n",
      "\n",
      "        [[ 1.1449e+00,  2.4455e-01,  1.5753e+00,  ..., -1.4303e+00,\n",
      "           9.5051e-03, -4.5678e-02],\n",
      "         [ 1.4372e+00,  5.0468e-01,  1.5669e+00,  ..., -1.7801e+00,\n",
      "          -3.9144e-01, -8.3864e-01],\n",
      "         [ 7.0025e-01,  3.9465e-01,  1.7570e+00,  ..., -1.6450e+00,\n",
      "          -4.3248e-01, -3.2521e-01],\n",
      "         [ 9.4737e-01,  5.6135e-01,  9.7441e-01,  ..., -2.0206e+00,\n",
      "           6.8170e-02, -5.0711e-01],\n",
      "         [ 8.4385e-01,  5.1096e-01,  6.2823e-01,  ..., -2.1546e+00,\n",
      "          -2.3753e-01, -7.6785e-01],\n",
      "         [ 5.9970e-01,  2.9736e-02,  3.4209e-01,  ..., -5.2601e-01,\n",
      "           8.6514e-01,  1.8015e-02]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.6131, -0.0857,  0.7147,  ..., -1.1047, -0.3107,  0.1120],\n",
      "         [ 0.5588, -1.1034,  0.2359,  ..., -1.6028, -1.2851, -0.3583],\n",
      "         [ 0.9314, -0.6718,  0.1734,  ..., -1.4037, -0.8899,  0.1537],\n",
      "         [ 0.1185, -0.0626,  0.6352,  ..., -0.3385, -0.7263,  0.1389],\n",
      "         [ 0.4894, -0.3713,  1.2345,  ..., -1.9709, -1.0296,  0.5189],\n",
      "         [ 0.1648, -0.1774, -0.0625,  ..., -1.1462, -0.9271,  0.2205]],\n",
      "\n",
      "        [[ 0.7413,  1.1187,  0.3677,  ...,  0.1652,  0.7703,  0.0639],\n",
      "         [ 0.4430, -0.7565,  0.9113,  ..., -0.0550, -0.1608, -0.5247],\n",
      "         [ 0.2965, -0.3599,  1.7663,  ...,  0.2681,  0.2717,  0.6741],\n",
      "         [ 0.7773, -0.4794,  1.0100,  ..., -0.4660,  0.1442, -0.2479],\n",
      "         [ 0.2274,  0.8332,  1.4863,  ..., -0.6592,  2.1463, -0.7029],\n",
      "         [ 0.9610,  0.5287,  1.8282,  ..., -0.3268,  0.6655,  0.0235]],\n",
      "\n",
      "        [[ 1.4518,  0.4295, -0.3086,  ..., -0.3309, -0.0195, -0.6563],\n",
      "         [ 1.5582, -0.7399, -0.2093,  ...,  0.3262,  0.0284,  0.3987],\n",
      "         [ 0.5824,  0.8282,  1.0734,  ...,  0.0132, -0.5942, -0.6722],\n",
      "         [ 0.5375,  0.4280,  0.6359,  ..., -0.5938,  0.0478,  0.0234],\n",
      "         [-0.2162,  0.8480,  0.8634,  ..., -0.4017, -0.3585, -0.3485],\n",
      "         [ 1.1981, -0.0978,  0.0923,  ...,  0.7481, -0.1732, -0.4493]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0200,  0.7430,  1.4204,  ..., -1.7147, -0.1470,  0.4597],\n",
      "         [ 1.1999,  0.6297,  1.6619,  ..., -1.7888, -0.3847, -0.2351],\n",
      "         [ 0.9215,  0.7231,  1.5006,  ..., -1.4548, -0.2079, -0.2268],\n",
      "         [ 0.5660,  0.4039,  1.8943,  ..., -2.0624, -0.0643, -0.3991],\n",
      "         [ 0.6561,  1.4412,  1.2387,  ..., -1.6845, -1.1043, -0.2106],\n",
      "         [ 0.7091,  0.4352,  2.9361,  ..., -0.3497, -0.0994, -0.4747]],\n",
      "\n",
      "        [[ 1.1172,  1.3628,  0.5227,  ..., -0.8518, -0.8384,  0.3970],\n",
      "         [ 1.0836,  0.8320,  1.1621,  ..., -1.3078, -1.0905,  0.1801],\n",
      "         [ 0.7725,  0.6120,  1.2109,  ..., -1.2911, -1.6212,  0.1534],\n",
      "         [ 0.4026,  1.0888,  0.5438,  ..., -0.9456, -0.2026,  0.3260],\n",
      "         [ 0.3679,  1.1268,  1.0613,  ..., -1.5245, -1.0461, -0.4236],\n",
      "         [ 1.5717,  0.6212,  0.7833,  ...,  0.1716, -0.4025,  0.3371]],\n",
      "\n",
      "        [[ 1.1472,  1.0976,  1.5863,  ..., -1.3125, -0.0390,  0.1954],\n",
      "         [ 1.4324,  1.2068,  1.4774,  ..., -1.6210, -0.7821, -0.5333],\n",
      "         [ 0.8323,  1.0077,  1.9609,  ..., -1.4098, -0.7428, -0.1435],\n",
      "         [ 1.0699,  1.3990,  0.8953,  ..., -1.8218,  0.0941, -0.7049],\n",
      "         [ 0.6867,  1.3300,  0.4087,  ..., -2.2631, -0.7020, -0.3807],\n",
      "         [ 0.4375,  0.2632,  0.4853,  ..., -0.2061,  0.3408,  0.2921]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 6.6686e-01, -4.8594e-01,  7.8279e-01,  ..., -7.0592e-01,\n",
      "          -7.3556e-01,  4.4880e-01],\n",
      "         [ 7.4359e-01, -1.2506e+00,  5.3238e-01,  ..., -1.8658e+00,\n",
      "          -1.6536e+00,  1.3173e-01],\n",
      "         [ 8.7092e-01, -1.0118e+00,  4.4033e-01,  ..., -1.2153e+00,\n",
      "          -1.1977e+00,  2.3910e-01],\n",
      "         [ 3.2349e-01, -3.0458e-01,  7.2769e-01,  ...,  2.8371e-02,\n",
      "          -1.0435e+00,  3.9582e-01],\n",
      "         [ 4.7667e-01, -3.4509e-01,  1.5168e+00,  ..., -1.8323e+00,\n",
      "          -1.1865e+00,  7.5241e-01],\n",
      "         [ 4.0925e-01, -7.3377e-01,  1.3495e-01,  ..., -1.0348e+00,\n",
      "          -1.2332e+00,  5.0521e-01]],\n",
      "\n",
      "        [[ 8.8695e-01,  5.3090e-01,  3.3088e-01,  ...,  6.4696e-01,\n",
      "           4.4508e-01,  4.2159e-01],\n",
      "         [ 6.0164e-01, -1.1952e+00,  7.2705e-01,  ...,  4.5529e-01,\n",
      "          -4.3529e-01, -1.6757e-01],\n",
      "         [ 5.2080e-01, -6.1177e-01,  1.6302e+00,  ...,  6.5514e-01,\n",
      "           5.0193e-02,  8.1759e-01],\n",
      "         [ 1.0014e+00, -7.1220e-01,  8.5620e-01,  ...,  1.5837e-01,\n",
      "          -3.4420e-02, -4.1778e-02],\n",
      "         [ 4.3005e-01,  8.9456e-01,  1.4572e+00,  ..., -4.2650e-01,\n",
      "           2.0851e+00, -3.5371e-01],\n",
      "         [ 1.1219e+00,  4.5793e-02,  1.7821e+00,  ...,  2.1635e-02,\n",
      "           5.4533e-01,  5.8354e-01]],\n",
      "\n",
      "        [[ 1.6411e+00,  4.4528e-02, -1.3439e-01,  ...,  1.3682e-01,\n",
      "          -4.1995e-01, -1.9074e-01],\n",
      "         [ 1.9583e+00, -1.0514e+00, -1.3280e-01,  ...,  7.0256e-01,\n",
      "          -6.0295e-01,  9.1159e-01],\n",
      "         [ 6.4847e-01,  4.6356e-01,  1.0401e+00,  ...,  3.9211e-01,\n",
      "          -7.6585e-01, -5.1933e-01],\n",
      "         [ 7.7478e-01,  1.9741e-01,  8.4105e-01,  ..., -5.5044e-02,\n",
      "          -4.4059e-01,  3.2856e-01],\n",
      "         [ 9.2094e-02,  5.7099e-01,  8.3848e-01,  ..., -9.8858e-02,\n",
      "          -4.1505e-01,  5.9236e-02],\n",
      "         [ 1.4079e+00, -6.4792e-01,  4.6344e-01,  ...,  1.1581e+00,\n",
      "          -6.0952e-01, -9.0615e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.4498e-02,  3.1059e-01,  8.5484e-01,  ..., -1.2114e+00,\n",
      "          -1.9853e-01,  9.1934e-01],\n",
      "         [ 1.3860e+00,  3.3904e-01,  1.8245e+00,  ..., -1.5590e+00,\n",
      "          -7.0525e-01, -3.2554e-01],\n",
      "         [ 1.1726e+00,  3.9770e-01,  1.2857e+00,  ..., -1.0906e+00,\n",
      "          -4.6722e-01,  5.2257e-02],\n",
      "         [ 6.2308e-01,  1.7043e-01,  1.7707e+00,  ..., -1.3834e+00,\n",
      "          -3.7542e-01, -2.2226e-03],\n",
      "         [ 6.4744e-01,  1.3464e+00,  1.2384e+00,  ..., -1.4488e+00,\n",
      "          -1.2404e+00,  2.1985e-01],\n",
      "         [ 1.2639e+00, -2.9069e-03,  2.7476e+00,  ..., -3.5717e-01,\n",
      "          -4.9406e-01, -2.0584e-02]],\n",
      "\n",
      "        [[ 1.0021e+00,  7.5152e-01,  3.7362e-01,  ..., -1.5962e-01,\n",
      "          -7.8888e-01,  5.1791e-01],\n",
      "         [ 1.1973e+00,  3.6660e-01,  9.5895e-01,  ..., -7.8033e-01,\n",
      "          -1.4571e+00,  5.9227e-01],\n",
      "         [ 8.0737e-01,  1.3501e-01,  1.1857e+00,  ..., -7.9825e-01,\n",
      "          -1.4614e+00,  4.4319e-01],\n",
      "         [ 4.9110e-01,  5.5872e-01,  4.5617e-01,  ..., -2.1181e-01,\n",
      "          -4.5686e-01,  3.4753e-01],\n",
      "         [ 2.3516e-01,  9.7292e-01,  8.6387e-01,  ..., -1.2488e+00,\n",
      "          -1.0916e+00, -8.5992e-02],\n",
      "         [ 1.6363e+00, -8.0326e-02,  6.1881e-01,  ...,  5.5516e-01,\n",
      "          -3.8022e-01,  6.6546e-01]],\n",
      "\n",
      "        [[ 1.1826e+00,  5.0171e-01,  1.5523e+00,  ..., -8.5457e-01,\n",
      "          -8.3723e-02,  6.6418e-01],\n",
      "         [ 1.4594e+00,  8.5924e-01,  1.3833e+00,  ..., -1.8008e+00,\n",
      "          -7.4816e-01, -4.4801e-02],\n",
      "         [ 6.6628e-01,  5.0667e-01,  1.8898e+00,  ..., -1.0879e+00,\n",
      "          -6.6776e-01,  4.1491e-02],\n",
      "         [ 1.1314e+00,  1.0687e+00,  6.7958e-01,  ..., -1.3176e+00,\n",
      "          -1.9253e-01, -4.4428e-01],\n",
      "         [ 7.2534e-01,  1.1371e+00,  2.8945e-01,  ..., -2.1337e+00,\n",
      "          -6.7615e-01,  1.7006e-01],\n",
      "         [ 7.1436e-01, -2.6283e-01,  4.8864e-01,  ..., -2.0451e-01,\n",
      "           1.3165e-01,  7.3801e-01]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 8.4639e-01, -4.0049e-03,  9.0335e-01,  ..., -1.5815e+00,\n",
      "          -1.6635e-01,  5.8414e-01],\n",
      "         [ 6.5765e-01, -7.1716e-01,  5.8741e-01,  ..., -2.4485e+00,\n",
      "          -1.2227e+00,  1.0953e-01],\n",
      "         [ 9.2815e-01, -5.1254e-01,  8.2282e-01,  ..., -2.0635e+00,\n",
      "          -8.5037e-01,  3.0066e-01],\n",
      "         [ 3.7968e-01,  1.8865e-01,  1.1280e+00,  ..., -8.8299e-01,\n",
      "          -6.8396e-01,  4.5387e-01],\n",
      "         [ 3.6938e-01,  7.7307e-02,  1.8824e+00,  ..., -2.4615e+00,\n",
      "          -7.0791e-01,  6.2468e-01],\n",
      "         [ 8.3715e-01, -1.4299e-01,  4.2857e-01,  ..., -2.0375e+00,\n",
      "          -5.3796e-01,  5.7610e-01]],\n",
      "\n",
      "        [[ 9.2483e-01,  8.8313e-01,  7.2614e-01,  ..., -4.5151e-01,\n",
      "           8.9279e-01,  8.0399e-01],\n",
      "         [ 5.1290e-01, -5.8958e-01,  1.1613e+00,  ..., -2.8900e-01,\n",
      "           2.5118e-01, -2.0338e-01],\n",
      "         [ 4.2823e-01, -3.4579e-02,  2.0159e+00,  ..., -2.8225e-01,\n",
      "           6.5298e-01,  1.0720e+00],\n",
      "         [ 5.9659e-01, -1.5746e-01,  1.4347e+00,  ..., -1.0223e+00,\n",
      "           3.9922e-01, -2.2260e-02],\n",
      "         [ 2.5512e-01,  1.0584e+00,  1.9241e+00,  ..., -1.5696e+00,\n",
      "           2.1347e+00, -4.9337e-01],\n",
      "         [ 1.1198e+00,  7.5585e-01,  2.2938e+00,  ..., -8.7384e-01,\n",
      "           1.0002e+00,  5.7188e-01]],\n",
      "\n",
      "        [[ 1.9337e+00,  4.2833e-01,  2.0329e-01,  ..., -8.0856e-01,\n",
      "           9.3918e-02,  2.9393e-01],\n",
      "         [ 1.7707e+00, -5.2456e-01,  2.6774e-01,  ..., -2.0203e-02,\n",
      "          -4.3811e-02,  9.7563e-01],\n",
      "         [ 5.5169e-01,  1.1196e+00,  1.5444e+00,  ..., -3.1363e-01,\n",
      "          -2.8795e-01, -1.2693e-01],\n",
      "         [ 7.9566e-01,  1.0081e+00,  1.0769e+00,  ..., -1.0363e+00,\n",
      "          -1.6591e-01,  6.3701e-01],\n",
      "         [ 1.7447e-01,  1.2769e+00,  1.3306e+00,  ..., -9.2588e-01,\n",
      "          -1.3823e-01,  1.2797e-01],\n",
      "         [ 1.3716e+00,  7.9458e-02,  7.2208e-01,  ...,  5.5233e-01,\n",
      "          -1.6959e-01,  2.8646e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.3242e-05,  5.2609e-01,  1.2513e+00,  ..., -1.9703e+00,\n",
      "           3.1816e-01,  1.4275e+00],\n",
      "         [ 1.3905e+00,  6.6070e-01,  1.9043e+00,  ..., -2.1230e+00,\n",
      "           1.1270e-02, -1.4875e-01],\n",
      "         [ 1.0127e+00,  9.6918e-01,  1.7052e+00,  ..., -1.8866e+00,\n",
      "          -4.1064e-01,  3.4947e-02],\n",
      "         [ 4.5041e-01,  9.8307e-01,  2.2259e+00,  ..., -2.1523e+00,\n",
      "           1.0096e-01,  1.5748e-01],\n",
      "         [ 6.2686e-01,  1.6660e+00,  9.8441e-01,  ..., -2.0754e+00,\n",
      "          -7.5481e-01,  4.6736e-01],\n",
      "         [ 1.3264e+00, -3.4549e-01,  2.8109e+00,  ..., -1.4116e+00,\n",
      "          -1.4427e-01,  2.3290e-03]],\n",
      "\n",
      "        [[ 1.0301e+00,  1.3149e+00,  5.7063e-01,  ..., -1.0633e+00,\n",
      "           1.2483e-03,  7.4078e-01],\n",
      "         [ 1.0048e+00,  8.4972e-01,  1.3228e+00,  ..., -1.0783e+00,\n",
      "          -4.5487e-01,  6.1093e-01],\n",
      "         [ 6.7453e-01,  6.9350e-01,  1.5363e+00,  ..., -1.6132e+00,\n",
      "          -6.5950e-01,  6.1382e-01],\n",
      "         [ 4.0097e-01,  1.1393e+00,  8.7255e-01,  ..., -8.6734e-01,\n",
      "           1.2846e-01,  4.3104e-01],\n",
      "         [ 2.6225e-01,  1.4328e+00,  1.3868e+00,  ..., -1.6548e+00,\n",
      "          -5.4476e-01, -9.5611e-02],\n",
      "         [ 1.5240e+00,  4.3282e-01,  1.1127e+00,  ..., -4.2314e-01,\n",
      "          -1.5973e-01,  9.9439e-01]],\n",
      "\n",
      "        [[ 1.0136e+00,  8.7464e-01,  1.3279e+00,  ..., -1.6597e+00,\n",
      "           6.1832e-01,  9.4110e-01],\n",
      "         [ 1.1126e+00,  1.4767e+00,  1.6557e+00,  ..., -2.2949e+00,\n",
      "           1.2108e-01,  2.6380e-01],\n",
      "         [ 3.3879e-01,  1.2217e+00,  2.2249e+00,  ..., -1.6789e+00,\n",
      "          -2.4225e-02,  1.7242e-01],\n",
      "         [ 8.5415e-01,  1.5803e+00,  8.9122e-01,  ..., -2.0367e+00,\n",
      "           2.8439e-01, -3.7527e-01],\n",
      "         [ 3.3767e-01,  7.8078e-01,  7.7608e-01,  ..., -2.7469e+00,\n",
      "          -1.3507e-01,  3.1776e-01],\n",
      "         [ 5.7539e-01,  3.2785e-01,  9.3855e-01,  ..., -1.1741e+00,\n",
      "           7.3512e-01,  9.4011e-01]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 9.4092e-01, -1.0330e+00,  7.6975e-01,  ..., -1.2875e+00,\n",
      "          -4.7889e-01,  4.2951e-01],\n",
      "         [ 7.6626e-01, -1.3624e+00,  5.3055e-01,  ..., -2.1515e+00,\n",
      "          -1.6899e+00,  1.0153e-02],\n",
      "         [ 1.0244e+00, -1.3687e+00,  5.2977e-01,  ..., -1.6132e+00,\n",
      "          -1.1830e+00,  1.4082e-01],\n",
      "         [ 6.1033e-01, -1.1033e+00,  1.1416e+00,  ..., -6.6669e-01,\n",
      "          -1.0274e+00,  4.6569e-01],\n",
      "         [ 6.1662e-01, -1.6931e-01,  1.3207e+00,  ..., -2.0554e+00,\n",
      "          -9.0281e-01,  6.6583e-01],\n",
      "         [ 9.5988e-01, -1.1874e+00,  2.7769e-01,  ..., -2.0127e+00,\n",
      "          -1.1575e+00,  3.5119e-01]],\n",
      "\n",
      "        [[ 1.0652e+00, -1.7295e-01,  6.7539e-01,  ..., -1.4804e-01,\n",
      "           2.8327e-01,  8.9454e-01],\n",
      "         [ 3.6987e-01, -1.2791e+00,  7.8439e-01,  ..., -1.2738e-01,\n",
      "          -4.9739e-01,  9.4745e-02],\n",
      "         [ 2.5811e-01, -7.7526e-01,  1.5524e+00,  ..., -3.7104e-01,\n",
      "           2.0772e-01,  1.2618e+00],\n",
      "         [ 6.0274e-01, -1.3103e+00,  1.1514e+00,  ..., -5.9923e-01,\n",
      "          -1.6121e-01,  7.4857e-02],\n",
      "         [-9.4651e-02, -1.0892e-01,  1.5014e+00,  ..., -1.1143e+00,\n",
      "           1.4472e+00, -2.7997e-01],\n",
      "         [ 9.1936e-01, -4.1108e-01,  2.1423e+00,  ..., -5.5841e-01,\n",
      "           2.4200e-01,  7.0607e-01]],\n",
      "\n",
      "        [[ 1.7328e+00,  3.7516e-01,  7.3549e-03,  ..., -7.7098e-01,\n",
      "          -4.3320e-01,  4.3308e-01],\n",
      "         [ 1.7206e+00, -1.5515e+00,  3.8779e-01,  ..., -8.3928e-02,\n",
      "          -6.3615e-01,  1.0746e+00],\n",
      "         [ 3.7833e-01, -6.4472e-02,  1.5853e+00,  ..., -2.2448e-01,\n",
      "          -7.8752e-01,  1.0584e-01],\n",
      "         [ 8.4414e-01, -4.4321e-01,  1.1792e+00,  ..., -7.3227e-01,\n",
      "          -4.2299e-01,  6.1964e-01],\n",
      "         [ 9.2981e-02,  1.3104e-01,  9.4162e-01,  ..., -1.0320e+00,\n",
      "          -5.9152e-01,  3.4441e-01],\n",
      "         [ 9.4507e-01, -1.1965e+00,  6.6830e-01,  ...,  6.2348e-01,\n",
      "          -9.2819e-01,  4.7151e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.8216e-01, -5.2683e-01,  1.0447e+00,  ..., -1.6171e+00,\n",
      "          -2.4099e-01,  1.3555e+00],\n",
      "         [ 1.0991e+00, -3.9852e-01,  1.5783e+00,  ..., -1.9823e+00,\n",
      "          -6.9395e-01,  2.0957e-03],\n",
      "         [ 6.9382e-01, -8.4758e-02,  1.3062e+00,  ..., -1.8396e+00,\n",
      "          -1.1062e+00,  1.7868e-01],\n",
      "         [ 1.0186e-01, -4.0391e-01,  1.9844e+00,  ..., -1.8177e+00,\n",
      "          -3.9190e-01,  4.1958e-01],\n",
      "         [ 5.5723e-02,  5.6773e-01,  5.9506e-01,  ..., -1.9442e+00,\n",
      "          -1.0412e+00,  7.3051e-01],\n",
      "         [ 1.0020e+00, -1.2336e+00,  2.4267e+00,  ..., -1.2416e+00,\n",
      "          -7.9430e-01,  5.4748e-02]],\n",
      "\n",
      "        [[ 8.3963e-01,  1.7841e-01,  5.9269e-01,  ..., -8.5159e-01,\n",
      "          -5.3163e-01,  6.8034e-01],\n",
      "         [ 8.1622e-01, -3.1410e-01,  1.1489e+00,  ..., -1.0241e+00,\n",
      "          -1.0199e+00,  5.7854e-01],\n",
      "         [ 5.8048e-01, -4.3789e-01,  1.2175e+00,  ..., -1.2259e+00,\n",
      "          -9.9309e-01,  5.4925e-01],\n",
      "         [ 2.1911e-01, -1.6627e-01,  8.5109e-01,  ..., -6.5609e-01,\n",
      "          -3.9662e-01,  4.9185e-01],\n",
      "         [-1.3917e-01,  2.2350e-01,  1.4232e+00,  ..., -1.4369e+00,\n",
      "          -1.0318e+00,  2.7565e-02],\n",
      "         [ 1.1127e+00, -7.0575e-01,  1.1688e+00,  ..., -1.8019e-01,\n",
      "          -7.6343e-01,  9.5916e-01]],\n",
      "\n",
      "        [[ 1.1618e+00, -1.0528e-01,  1.5305e+00,  ..., -1.5160e+00,\n",
      "          -3.6351e-01,  8.4720e-01],\n",
      "         [ 1.1205e+00,  7.0285e-02,  1.9180e+00,  ..., -2.1688e+00,\n",
      "          -6.6835e-01,  5.9222e-01],\n",
      "         [ 4.0865e-01,  1.3852e-01,  1.9034e+00,  ..., -1.6260e+00,\n",
      "          -9.0123e-01,  3.9306e-01],\n",
      "         [ 9.8936e-01,  1.7304e-01,  9.0804e-01,  ..., -1.8927e+00,\n",
      "          -5.9217e-01, -6.5608e-01],\n",
      "         [ 3.4757e-01, -4.5329e-01,  4.3083e-01,  ..., -2.4261e+00,\n",
      "          -8.6755e-01,  3.9603e-01],\n",
      "         [ 5.1004e-01, -6.8819e-01,  9.2364e-01,  ..., -1.2047e+00,\n",
      "          -3.4577e-02,  1.1338e+00]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.8360, -0.7751,  0.9359,  ..., -0.9583, -0.6086,  0.3941],\n",
      "         [ 0.5009, -1.2277,  0.6185,  ..., -1.4839, -1.5456, -0.6804],\n",
      "         [ 0.9924, -1.0723,  0.9036,  ..., -1.2793, -1.4000, -0.0591],\n",
      "         [ 0.5009, -0.7969,  1.2246,  ...,  0.0996, -1.1723, -0.1119],\n",
      "         [ 0.5880,  0.3705,  1.7993,  ..., -1.3708, -0.5975,  0.1182],\n",
      "         [ 0.9808, -1.0714,  0.4009,  ..., -1.2019, -1.1730, -0.0731]],\n",
      "\n",
      "        [[ 1.0287, -0.1733,  0.8973,  ...,  0.1939,  0.2911,  0.2114],\n",
      "         [ 0.5060, -1.5512,  0.7384,  ...,  0.2599, -0.5595, -0.4681],\n",
      "         [ 0.4997, -0.9142,  2.1503,  ...,  0.0088,  0.1998,  0.8011],\n",
      "         [ 0.6989, -1.2884,  1.4378,  ..., -0.4433, -0.2191, -0.4134],\n",
      "         [ 0.3871,  0.2485,  1.8781,  ..., -0.5458,  1.2952, -0.5775],\n",
      "         [ 1.0230, -0.5552,  2.3113,  ..., -0.1543, -0.2122, -0.0196]],\n",
      "\n",
      "        [[ 1.6776,  0.3385,  0.6658,  ..., -0.3719, -0.7599, -0.2237],\n",
      "         [ 1.8110, -1.6436,  0.7937,  ...,  0.4706, -0.7886, -0.0170],\n",
      "         [ 0.4607, -0.1943,  1.6999,  ...,  0.1286, -1.2173, -0.5150],\n",
      "         [ 0.9152, -0.5813,  1.3734,  ..., -0.1720, -0.5268, -0.0707],\n",
      "         [ 0.3628,  0.3599,  1.5208,  ..., -0.5873, -0.6103, -0.5116],\n",
      "         [ 1.0482, -1.1372,  0.9387,  ...,  0.9985, -1.1644, -0.3809]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.4103, -0.3230,  1.4434,  ..., -1.2646, -0.4636,  0.6287],\n",
      "         [ 0.8156, -0.3349,  1.8157,  ..., -1.3866, -0.5159, -0.2181],\n",
      "         [ 0.8990, -0.0534,  1.6014,  ..., -1.3750, -1.4043,  0.0275],\n",
      "         [ 0.1231, -0.3309,  1.9337,  ..., -1.3652, -0.5969, -0.2649],\n",
      "         [ 0.3405,  1.0484,  1.0343,  ..., -1.3728, -1.0234, -0.0527],\n",
      "         [ 1.4321, -1.1026,  2.0984,  ..., -0.8762, -1.0472, -0.4560]],\n",
      "\n",
      "        [[ 0.9658,  0.2253,  0.9394,  ..., -0.3433, -0.7094,  0.2052],\n",
      "         [ 0.8680, -0.5304,  1.3549,  ..., -0.1845, -1.3118, -0.2885],\n",
      "         [ 0.9105, -0.4884,  1.6053,  ..., -0.5034, -1.1601, -0.2388],\n",
      "         [ 0.2055, -0.2433,  1.1554,  ..., -0.0478, -0.5767, -0.4890],\n",
      "         [-0.1729,  0.5965,  1.7881,  ..., -0.8426, -1.0117, -0.6012],\n",
      "         [ 1.1720, -0.6278,  1.3165,  ...,  0.1534, -1.1888,  0.2712]],\n",
      "\n",
      "        [[ 1.1286, -0.1253,  1.9759,  ..., -1.1420, -0.4242,  0.0823],\n",
      "         [ 1.1320, -0.3149,  2.1695,  ..., -1.3591, -0.9558, -0.4544],\n",
      "         [ 0.5147,  0.0345,  2.3651,  ..., -1.1217, -1.0837, -0.2019],\n",
      "         [ 0.9284,  0.0791,  0.8415,  ..., -1.2215, -0.4025, -1.3623],\n",
      "         [ 0.5903,  0.0187,  0.6571,  ..., -1.7525, -0.4473,  0.1249],\n",
      "         [ 0.3335, -0.7487,  1.0489,  ..., -0.7799, -0.4713,  0.4822]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.2908, -0.8230,  0.7287,  ..., -0.9899, -0.8779, -0.3096],\n",
      "         [-0.1621, -1.0944,  0.4892,  ..., -1.4018, -1.7689, -0.9198],\n",
      "         [ 0.1911, -1.0898,  0.9412,  ..., -1.3924, -1.1779, -0.4340],\n",
      "         [ 0.0274, -0.8003,  1.0184,  ...,  0.3790, -1.2299, -0.6630],\n",
      "         [ 0.5387,  0.4361,  1.4450,  ..., -1.4708, -0.6724, -0.3936],\n",
      "         [ 0.3099, -0.9032,  0.2924,  ..., -1.2175, -1.2574, -0.6135]],\n",
      "\n",
      "        [[ 0.1542, -0.2189,  0.7938,  ..., -0.0761, -0.0566, -0.5655],\n",
      "         [-0.2628, -1.3269,  0.6165,  ...,  0.1393, -0.9044, -0.9715],\n",
      "         [-0.1222, -0.8685,  2.0249,  ...,  0.2566,  0.1834,  0.1802],\n",
      "         [ 0.1096, -1.1022,  1.3094,  ..., -0.5485, -0.1002, -0.4656],\n",
      "         [-0.6182,  0.2724,  1.7012,  ..., -0.5392,  1.1971, -1.0330],\n",
      "         [ 0.2100, -0.4890,  1.8526,  ..., -0.2078, -0.3995, -0.6310]],\n",
      "\n",
      "        [[ 1.3359,  0.1288,  0.7433,  ..., -0.3716, -0.9683, -0.8296],\n",
      "         [ 1.1198, -1.6229,  0.6264,  ...,  0.1646, -1.0097, -0.3651],\n",
      "         [-0.0770, -0.2185,  2.0944,  ..., -0.2329, -1.2186, -0.5021],\n",
      "         [ 0.1846, -0.5599,  1.2879,  ..., -0.3130, -0.4838, -0.6548],\n",
      "         [-0.5054,  0.5055,  1.4016,  ..., -0.7194, -0.7119, -0.9596],\n",
      "         [ 0.4041, -0.9616,  0.7894,  ...,  0.9665, -1.3697, -0.7396]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.6143, -0.3724,  1.0281,  ..., -1.1566, -0.6572,  0.5474],\n",
      "         [ 0.7571, -0.3178,  1.5773,  ..., -1.2310, -0.6883, -0.6205],\n",
      "         [-0.0675,  0.0386,  1.5971,  ..., -0.9941, -1.2634, -0.3587],\n",
      "         [-0.6521, -0.0108,  1.5456,  ..., -1.2071, -0.5833, -0.7313],\n",
      "         [-0.3549,  1.1004,  0.9439,  ..., -1.3667, -0.9939,  0.1000],\n",
      "         [ 0.4675, -1.0471,  1.9974,  ..., -1.0718, -1.1770, -0.8609]],\n",
      "\n",
      "        [[ 0.0139,  0.2078,  0.9811,  ..., -0.2462, -0.8762, -0.3249],\n",
      "         [ 0.0164, -0.3450,  1.2984,  ..., -0.1436, -1.5432, -0.7119],\n",
      "         [-0.0639, -0.5887,  1.4956,  ..., -0.6835, -0.8908, -0.6591],\n",
      "         [-0.7034, -0.1618,  1.0430,  ...,  0.0282, -0.5755, -0.8839],\n",
      "         [-1.4126,  0.6136,  1.4764,  ..., -0.6155, -1.2615, -1.1035],\n",
      "         [ 0.7501, -0.2568,  1.2774,  ...,  0.2225, -1.3968, -0.2904]],\n",
      "\n",
      "        [[ 0.1633, -0.3153,  1.7963,  ..., -1.1191, -0.6633,  0.1634],\n",
      "         [ 0.3383, -0.2379,  1.9214,  ..., -1.1646, -1.1647, -0.9681],\n",
      "         [-0.2620,  0.0419,  2.3329,  ..., -0.9963, -0.8865, -0.5996],\n",
      "         [ 0.0563,  0.1089,  0.5640,  ..., -0.9227, -0.3346, -1.8545],\n",
      "         [-0.4291,  0.1232,  0.5543,  ..., -1.6849, -0.6178, -0.4958],\n",
      "         [-0.3110, -0.7872,  1.1415,  ..., -0.5108, -0.5862, -0.1789]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "bert_attention = BertAttention(config)\n",
    "bert_attention_output = bert_attention(bert_embedding_result, attention_mask=attention_mask)\n",
    "print(f\"BertAttention output shape [src_len, batch_size, hidden_size]: \", bert_attention_output.shape)\n",
    "\n",
    "bert_layer = BertLayer(config)\n",
    "bert_layer_output = bert_layer(bert_embedding_result, attention_mask)\n",
    "\n",
    "bert_encoder = BertEncoder(config)\n",
    "bert_encoder_outputs = bert_encoder(bert_embedding_result, attention_mask)\n",
    "print(f\"num of BertEncoder [config.num_hidden_layers]: \", len(bert_encoder_outputs))\n",
    "print(f\"each output shape in BertEncoder [src_len, batch_size, hidden_size]: \", bert_encoder_outputs[0].shape)\n",
    "print(bert_encoder_outputs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**This snippet demonstrates how BERT’s final pooled representations are fed into the classification head, and how to visualize the model graph with TensorBoard:**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 16])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from model import BertForSentenceClassification\n",
    "config.__dict__['num_labels'] = 16\n",
    "config.__dict__['num_hidden_layers'] = 3\n",
    "model = BertForSentenceClassification(config)\n",
    "\n",
    "input_ids = src\n",
    "attention_mask =mask  # [batch_size,src_len]\n",
    "logits = model(input_ids=input_ids,\n",
    "                   attention_mask=attention_mask)\n",
    "print(logits.shape)\n",
    "writer = SummaryWriter('./runs')\n",
    "writer.add_graph(model, input_ids)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "Launching TensorBoard..."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir runs --port 6006"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pretrained model loading\n",
    "### My BERT Model Parameters Inspection\n",
    "This code snippet loads the BERT configuration and instantiates the `BertModel` to inspect its learnable parameters:\n",
    "\n",
    "1. **Configuration Loading**\n",
    "   - Reads hyperparameters such as `hidden_size`, `num_hidden_layers`, and `vocab_size` from `config.json`.\n",
    "2. **Model Initialization**\n",
    "   - Calls `BertModel(config)` to build the full model architecture, including embeddings, encoder layers, and pooler.\n",
    "3. **Parameter Enumeration**\n",
    "   - `state_dict()` returns a mapping of parameter names to tensors (weights and biases).\n",
    "   - `len(state_dict)` gives the total count of parameter tensors in the model.\n",
    "   - Iterating through `state_dict.items()` prints each parameter’s name and its shape, providing insight into layer dimensions and verifying alignment with BERT’s base configuration.\n",
    "\n",
    "This inspection is useful for debugging custom implementations and ensuring that parameter counts and shapes match expected values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  =======  MyBert Parameters: ========\n",
      "200\n",
      "bert_embeddings.position_ids \t torch.Size([1, 512])\n",
      "bert_embeddings.word_embeddings.embedding.weight \t torch.Size([21128, 768])\n",
      "bert_embeddings.position_embeddings.embedding.weight \t torch.Size([512, 768])\n",
      "bert_embeddings.token_type_embeddings.embedding.weight \t torch.Size([2, 768])\n",
      "bert_embeddings.LayerNorm.weight \t torch.Size([768])\n",
      "bert_embeddings.LayerNorm.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.0.bert_attention.self.multi_head_attention.q_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.0.bert_attention.self.multi_head_attention.q_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.0.bert_attention.self.multi_head_attention.k_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.0.bert_attention.self.multi_head_attention.k_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.0.bert_attention.self.multi_head_attention.v_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.0.bert_attention.self.multi_head_attention.v_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.0.bert_attention.self.multi_head_attention.out_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.0.bert_attention.self.multi_head_attention.out_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.0.bert_attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert_encoder.bert_layers.0.bert_attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.0.bert_intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert_encoder.bert_layers.0.bert_intermediate.dense.bias \t torch.Size([3072])\n",
      "bert_encoder.bert_layers.0.bert_output.dense.weight \t torch.Size([768, 3072])\n",
      "bert_encoder.bert_layers.0.bert_output.dense.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.0.bert_output.LayerNorm.weight \t torch.Size([768])\n",
      "bert_encoder.bert_layers.0.bert_output.LayerNorm.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.1.bert_attention.self.multi_head_attention.q_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.1.bert_attention.self.multi_head_attention.q_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.1.bert_attention.self.multi_head_attention.k_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.1.bert_attention.self.multi_head_attention.k_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.1.bert_attention.self.multi_head_attention.v_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.1.bert_attention.self.multi_head_attention.v_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.1.bert_attention.self.multi_head_attention.out_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.1.bert_attention.self.multi_head_attention.out_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.1.bert_attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert_encoder.bert_layers.1.bert_attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.1.bert_intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert_encoder.bert_layers.1.bert_intermediate.dense.bias \t torch.Size([3072])\n",
      "bert_encoder.bert_layers.1.bert_output.dense.weight \t torch.Size([768, 3072])\n",
      "bert_encoder.bert_layers.1.bert_output.dense.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.1.bert_output.LayerNorm.weight \t torch.Size([768])\n",
      "bert_encoder.bert_layers.1.bert_output.LayerNorm.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.2.bert_attention.self.multi_head_attention.q_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.2.bert_attention.self.multi_head_attention.q_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.2.bert_attention.self.multi_head_attention.k_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.2.bert_attention.self.multi_head_attention.k_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.2.bert_attention.self.multi_head_attention.v_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.2.bert_attention.self.multi_head_attention.v_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.2.bert_attention.self.multi_head_attention.out_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.2.bert_attention.self.multi_head_attention.out_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.2.bert_attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert_encoder.bert_layers.2.bert_attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.2.bert_intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert_encoder.bert_layers.2.bert_intermediate.dense.bias \t torch.Size([3072])\n",
      "bert_encoder.bert_layers.2.bert_output.dense.weight \t torch.Size([768, 3072])\n",
      "bert_encoder.bert_layers.2.bert_output.dense.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.2.bert_output.LayerNorm.weight \t torch.Size([768])\n",
      "bert_encoder.bert_layers.2.bert_output.LayerNorm.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.3.bert_attention.self.multi_head_attention.q_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.3.bert_attention.self.multi_head_attention.q_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.3.bert_attention.self.multi_head_attention.k_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.3.bert_attention.self.multi_head_attention.k_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.3.bert_attention.self.multi_head_attention.v_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.3.bert_attention.self.multi_head_attention.v_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.3.bert_attention.self.multi_head_attention.out_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.3.bert_attention.self.multi_head_attention.out_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.3.bert_attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert_encoder.bert_layers.3.bert_attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.3.bert_intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert_encoder.bert_layers.3.bert_intermediate.dense.bias \t torch.Size([3072])\n",
      "bert_encoder.bert_layers.3.bert_output.dense.weight \t torch.Size([768, 3072])\n",
      "bert_encoder.bert_layers.3.bert_output.dense.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.3.bert_output.LayerNorm.weight \t torch.Size([768])\n",
      "bert_encoder.bert_layers.3.bert_output.LayerNorm.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.4.bert_attention.self.multi_head_attention.q_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.4.bert_attention.self.multi_head_attention.q_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.4.bert_attention.self.multi_head_attention.k_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.4.bert_attention.self.multi_head_attention.k_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.4.bert_attention.self.multi_head_attention.v_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.4.bert_attention.self.multi_head_attention.v_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.4.bert_attention.self.multi_head_attention.out_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.4.bert_attention.self.multi_head_attention.out_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.4.bert_attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert_encoder.bert_layers.4.bert_attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.4.bert_intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert_encoder.bert_layers.4.bert_intermediate.dense.bias \t torch.Size([3072])\n",
      "bert_encoder.bert_layers.4.bert_output.dense.weight \t torch.Size([768, 3072])\n",
      "bert_encoder.bert_layers.4.bert_output.dense.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.4.bert_output.LayerNorm.weight \t torch.Size([768])\n",
      "bert_encoder.bert_layers.4.bert_output.LayerNorm.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.5.bert_attention.self.multi_head_attention.q_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.5.bert_attention.self.multi_head_attention.q_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.5.bert_attention.self.multi_head_attention.k_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.5.bert_attention.self.multi_head_attention.k_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.5.bert_attention.self.multi_head_attention.v_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.5.bert_attention.self.multi_head_attention.v_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.5.bert_attention.self.multi_head_attention.out_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.5.bert_attention.self.multi_head_attention.out_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.5.bert_attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert_encoder.bert_layers.5.bert_attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.5.bert_intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert_encoder.bert_layers.5.bert_intermediate.dense.bias \t torch.Size([3072])\n",
      "bert_encoder.bert_layers.5.bert_output.dense.weight \t torch.Size([768, 3072])\n",
      "bert_encoder.bert_layers.5.bert_output.dense.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.5.bert_output.LayerNorm.weight \t torch.Size([768])\n",
      "bert_encoder.bert_layers.5.bert_output.LayerNorm.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.6.bert_attention.self.multi_head_attention.q_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.6.bert_attention.self.multi_head_attention.q_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.6.bert_attention.self.multi_head_attention.k_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.6.bert_attention.self.multi_head_attention.k_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.6.bert_attention.self.multi_head_attention.v_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.6.bert_attention.self.multi_head_attention.v_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.6.bert_attention.self.multi_head_attention.out_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.6.bert_attention.self.multi_head_attention.out_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.6.bert_attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert_encoder.bert_layers.6.bert_attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.6.bert_intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert_encoder.bert_layers.6.bert_intermediate.dense.bias \t torch.Size([3072])\n",
      "bert_encoder.bert_layers.6.bert_output.dense.weight \t torch.Size([768, 3072])\n",
      "bert_encoder.bert_layers.6.bert_output.dense.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.6.bert_output.LayerNorm.weight \t torch.Size([768])\n",
      "bert_encoder.bert_layers.6.bert_output.LayerNorm.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.7.bert_attention.self.multi_head_attention.q_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.7.bert_attention.self.multi_head_attention.q_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.7.bert_attention.self.multi_head_attention.k_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.7.bert_attention.self.multi_head_attention.k_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.7.bert_attention.self.multi_head_attention.v_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.7.bert_attention.self.multi_head_attention.v_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.7.bert_attention.self.multi_head_attention.out_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.7.bert_attention.self.multi_head_attention.out_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.7.bert_attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert_encoder.bert_layers.7.bert_attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.7.bert_intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert_encoder.bert_layers.7.bert_intermediate.dense.bias \t torch.Size([3072])\n",
      "bert_encoder.bert_layers.7.bert_output.dense.weight \t torch.Size([768, 3072])\n",
      "bert_encoder.bert_layers.7.bert_output.dense.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.7.bert_output.LayerNorm.weight \t torch.Size([768])\n",
      "bert_encoder.bert_layers.7.bert_output.LayerNorm.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.8.bert_attention.self.multi_head_attention.q_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.8.bert_attention.self.multi_head_attention.q_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.8.bert_attention.self.multi_head_attention.k_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.8.bert_attention.self.multi_head_attention.k_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.8.bert_attention.self.multi_head_attention.v_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.8.bert_attention.self.multi_head_attention.v_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.8.bert_attention.self.multi_head_attention.out_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.8.bert_attention.self.multi_head_attention.out_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.8.bert_attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert_encoder.bert_layers.8.bert_attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.8.bert_intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert_encoder.bert_layers.8.bert_intermediate.dense.bias \t torch.Size([3072])\n",
      "bert_encoder.bert_layers.8.bert_output.dense.weight \t torch.Size([768, 3072])\n",
      "bert_encoder.bert_layers.8.bert_output.dense.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.8.bert_output.LayerNorm.weight \t torch.Size([768])\n",
      "bert_encoder.bert_layers.8.bert_output.LayerNorm.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.9.bert_attention.self.multi_head_attention.q_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.9.bert_attention.self.multi_head_attention.q_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.9.bert_attention.self.multi_head_attention.k_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.9.bert_attention.self.multi_head_attention.k_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.9.bert_attention.self.multi_head_attention.v_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.9.bert_attention.self.multi_head_attention.v_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.9.bert_attention.self.multi_head_attention.out_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.9.bert_attention.self.multi_head_attention.out_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.9.bert_attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert_encoder.bert_layers.9.bert_attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.9.bert_intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert_encoder.bert_layers.9.bert_intermediate.dense.bias \t torch.Size([3072])\n",
      "bert_encoder.bert_layers.9.bert_output.dense.weight \t torch.Size([768, 3072])\n",
      "bert_encoder.bert_layers.9.bert_output.dense.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.9.bert_output.LayerNorm.weight \t torch.Size([768])\n",
      "bert_encoder.bert_layers.9.bert_output.LayerNorm.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.10.bert_attention.self.multi_head_attention.q_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.10.bert_attention.self.multi_head_attention.q_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.10.bert_attention.self.multi_head_attention.k_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.10.bert_attention.self.multi_head_attention.k_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.10.bert_attention.self.multi_head_attention.v_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.10.bert_attention.self.multi_head_attention.v_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.10.bert_attention.self.multi_head_attention.out_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.10.bert_attention.self.multi_head_attention.out_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.10.bert_attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert_encoder.bert_layers.10.bert_attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.10.bert_intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert_encoder.bert_layers.10.bert_intermediate.dense.bias \t torch.Size([3072])\n",
      "bert_encoder.bert_layers.10.bert_output.dense.weight \t torch.Size([768, 3072])\n",
      "bert_encoder.bert_layers.10.bert_output.dense.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.10.bert_output.LayerNorm.weight \t torch.Size([768])\n",
      "bert_encoder.bert_layers.10.bert_output.LayerNorm.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.11.bert_attention.self.multi_head_attention.q_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.11.bert_attention.self.multi_head_attention.q_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.11.bert_attention.self.multi_head_attention.k_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.11.bert_attention.self.multi_head_attention.k_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.11.bert_attention.self.multi_head_attention.v_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.11.bert_attention.self.multi_head_attention.v_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.11.bert_attention.self.multi_head_attention.out_proj.weight \t torch.Size([768, 768])\n",
      "bert_encoder.bert_layers.11.bert_attention.self.multi_head_attention.out_proj.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.11.bert_attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "bert_encoder.bert_layers.11.bert_attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.11.bert_intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "bert_encoder.bert_layers.11.bert_intermediate.dense.bias \t torch.Size([3072])\n",
      "bert_encoder.bert_layers.11.bert_output.dense.weight \t torch.Size([768, 3072])\n",
      "bert_encoder.bert_layers.11.bert_output.dense.bias \t torch.Size([768])\n",
      "bert_encoder.bert_layers.11.bert_output.LayerNorm.weight \t torch.Size([768])\n",
      "bert_encoder.bert_layers.11.bert_output.LayerNorm.bias \t torch.Size([768])\n",
      "bert_pooler.dense.weight \t torch.Size([768, 768])\n",
      "bert_pooler.dense.bias \t torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "json_file = 'bert_base_chinese/config.json'\n",
    "config = BertConfig.from_json_file(json_file)\n",
    "bert_model = BertModel(config)\n",
    "print(\"\\n  =======  MyBert Parameters: ========\")\n",
    "print(len(bert_model.state_dict()))\n",
    "for param_tensor in bert_model.state_dict():\n",
    "    print(param_tensor, \"\\t\", bert_model.state_dict()[param_tensor].size())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pretrained BERT Model Parameters Inspection\n",
    "This code snippet loads the bert-base chinese BERT configuration to inspect its learnable parameters:\n",
    "\n",
    "1. **Configuration Loading**\n",
    "   - Reads hyperparameters such as `hidden_size`, `num_hidden_layers`, and `vocab_size` from `/pytorch_model.bin`.\n",
    "   - The parameter pytorch_model.bin is loaded as an ordered dictionary OrderedDict, and there are 207 parameters whose names are the elements of the list.\n",
    "2. **Comparison**\n",
    "   - My Bert has a total of 200 parameters, while bert-base-chinese has a total of 207 parameters.It should be noted here that the parameter position_ids in my bert model is not a parameter that needs to be trained in the model, it is just a default initial value.Finally, after analysing (comparing the two one by one), we found that the 199 parameters in bert-base-chinese are the same as the 199 parameters in my bert model and in the same order, except for the last 8 parameters.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.OrderedDict'>\n",
      "207\n",
      "['bert.embeddings.word_embeddings.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.LayerNorm.gamma', 'bert.embeddings.LayerNorm.beta', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.gamma', 'bert.encoder.layer.0.attention.output.LayerNorm.beta', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.gamma', 'bert.encoder.layer.0.output.LayerNorm.beta', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.gamma', 'bert.encoder.layer.1.attention.output.LayerNorm.beta', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.gamma', 'bert.encoder.layer.1.output.LayerNorm.beta', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.gamma', 'bert.encoder.layer.2.attention.output.LayerNorm.beta', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.gamma', 'bert.encoder.layer.2.output.LayerNorm.beta', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.gamma', 'bert.encoder.layer.3.attention.output.LayerNorm.beta', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.LayerNorm.gamma', 'bert.encoder.layer.3.output.LayerNorm.beta', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.gamma', 'bert.encoder.layer.4.attention.output.LayerNorm.beta', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.gamma', 'bert.encoder.layer.4.output.LayerNorm.beta', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.gamma', 'bert.encoder.layer.5.attention.output.LayerNorm.beta', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.gamma', 'bert.encoder.layer.5.output.LayerNorm.beta', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.gamma', 'bert.encoder.layer.6.attention.output.LayerNorm.beta', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.gamma', 'bert.encoder.layer.6.output.LayerNorm.beta', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.gamma', 'bert.encoder.layer.7.attention.output.LayerNorm.beta', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.gamma', 'bert.encoder.layer.7.output.LayerNorm.beta', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.gamma', 'bert.encoder.layer.8.attention.output.LayerNorm.beta', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.gamma', 'bert.encoder.layer.8.output.LayerNorm.beta', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.gamma', 'bert.encoder.layer.9.attention.output.LayerNorm.beta', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.gamma', 'bert.encoder.layer.9.output.LayerNorm.beta', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.gamma', 'bert.encoder.layer.10.attention.output.LayerNorm.beta', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.gamma', 'bert.encoder.layer.10.output.LayerNorm.beta', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.gamma', 'bert.encoder.layer.11.attention.output.LayerNorm.beta', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.LayerNorm.gamma', 'bert.encoder.layer.11.output.LayerNorm.beta', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.gamma', 'cls.predictions.transform.LayerNorm.beta', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
     ]
    }
   ],
   "source": [
    "loaded_paras = torch.load('bert_base_chinese/pytorch_model.bin')\n",
    "print(type(loaded_paras))\n",
    "print(len(list(loaded_paras.keys())))\n",
    "print(list(loaded_paras.keys()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pretrained Parameter Loading (`from_pretrained`)\n",
    "This code illustrates how BERT’s pretrained weights are loaded into a custom model implementation while handling differences in attention mechanics and position embedding sizes:\n",
    "\n",
    "1. **Model Instantiation**\n",
    "   - `cls(config)` creates a fresh `BertModel` with randomly initialized parameters based on the given `config`.\n",
    "2. **Checkpoint Path Resolution**\n",
    "   - Constructs `pytorch_model.bin` path under `pretrained_model_dir`. Raises an error if the file is missing.\n",
    "3. **Parameter Extraction**\n",
    "   - Loads the binary checkpoint into `loaded_paras`, containing state dictionaries from the original BERT pretraining.\n",
    "   - Separates out the last pooler parameters (skipped via `[:-8]`).\n",
    "4. **Torch Multi-Head Handling**\n",
    "   - If `config.use_torch_multi_head` is `True`, merges separate query/key/value weight tensors into one concatenated tensor via `format_paras_for_torch` to match `nn.MultiheadAttention` expectations.\n",
    "5. **Position Embedding Extension**\n",
    "   - Checks for `position_embeddings` parameters. If `max_position_embeddings > 512`, applies `replace_512_position` to extend the embedding table beyond the original 512 tokens, preserving pretrained values.\n",
    "6. **State Dict Assignment**\n",
    "   - Iterates through the custom model’s parameter names, assigning each with the corresponding tensor from `loaded_paras` or the processed `torch_paras`.\n",
    "7. **Finalization**\n",
    "   - Loads the assembled `state_dict` into the model via `load_state_dict`, completing the injection of pretrained weights.\n",
    "\n",
    "This procedure ensures compatibility between pretrained BERT checkpoints and custom implementations, even when extending sequence length or switching attention backends."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  =======  test BertModel pretrained： ========\n"
     ]
    },
    {
     "data": {
      "text/plain": "BertModel(\n  (bert_embeddings): BertEmbeddings(\n    (word_embeddings): TokenEmbedding(\n      (embedding): Embedding(21128, 768)\n    )\n    (position_embeddings): PositionalEmbedding(\n      (embedding): Embedding(512, 768)\n    )\n    (token_type_embeddings): SegmentEmbedding(\n      (embedding): Embedding(2, 768)\n    )\n    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (bert_encoder): BertEncoder(\n    (bert_layers): ModuleList(\n      (0-11): 12 x BertLayer(\n        (bert_attention): BertAttention(\n          (self): BertSelfAttention(\n            (multi_head_attention): MyMultiheadAttention(\n              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n            )\n          )\n          (output): BertSelfOutput(\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (bert_intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELU(approximate='none')\n        )\n        (bert_output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (bert_pooler): BertPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import os\n",
    "from model.BasicBert.Bert import format_paras_for_torch, replace_512_position\n",
    "\n",
    "\n",
    "def from_pretrained(cls, config, pretrained_model_dir=None):\n",
    "    model = cls(config)  # 初始化模型，cls为未实例化的对象，即一个未实例化的BertModel对象\n",
    "    pretrained_model_path = os.path.join(pretrained_model_dir, \"pytorch_model.bin\")\n",
    "    if not os.path.exists(pretrained_model_path):\n",
    "        raise ValueError()\n",
    "    loaded_paras = torch.load(pretrained_model_path,weights_only=True)\n",
    "    state_dict = deepcopy(model.state_dict())\n",
    "    loaded_paras_names = list(loaded_paras.keys())[:-8]\n",
    "    model_paras_names = list(state_dict.keys())[1:]\n",
    "    if 'use_torch_multi_head' in config.__dict__ and config.use_torch_multi_head:\n",
    "        torch_paras = format_paras_for_torch(loaded_paras_names, loaded_paras)\n",
    "        for i in range(len(model_paras_names)):\n",
    "            if \"position_embeddings\" in model_paras_names[i]:\n",
    "                if config.max_position_embeddings > 512:\n",
    "                    new_embedding = replace_512_position(state_dict[model_paras_names[i]],\n",
    "                                                             loaded_paras[loaded_paras_names[i]])\n",
    "                    state_dict[model_paras_names[i]] = new_embedding\n",
    "                    continue\n",
    "            state_dict[model_paras_names[i]] = torch_paras[i]\n",
    "    else:\n",
    "        for i in range(len(loaded_paras_names)):\n",
    "            if \"position_embeddings\" in model_paras_names[i]:\n",
    "                if config.max_position_embeddings > 512:\n",
    "                    new_embedding = replace_512_position(state_dict[model_paras_names[i]],\n",
    "                                                             loaded_paras[loaded_paras_names[i]])\n",
    "                    state_dict[model_paras_names[i]] = new_embedding\n",
    "                    continue\n",
    "            state_dict[model_paras_names[i]] = loaded_paras[loaded_paras_names[i]]\n",
    "\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model\n",
    "print(f\"\\n  =======  test BertModel pretrained： ========\")\n",
    "model = BertModel.from_pretrained(config, pretrained_model_dir=\"bert_base_chinese\")\n",
    "model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Semantic loss\n",
    "This function computes an additional loss to guide the model's learning of word semantics,using external knowledge from HowNet (a Chinese lexical resource). The goal is to encourage words with similar meanings (synonyms) to have similar representations, and words with opposite meanings (antonyms) to have dissimilar representations.\n",
    "The semantic loss consists of two parts:\n",
    "   1. **Synonym Loss**: Uses Mean Squared Error (MSE) to minimize the difference between the\n",
    "       embeddings of synonyms, encouraging them to have similar representations.\n",
    "   2. **Antonym Loss**: Uses margin-based loss to increase the distance between the embeddings\n",
    "       of antonyms, encouraging them to have distinct representations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "def compute_semantic_loss(input_ids, hidden_states, vocab, lexicon, lamda=0.1, margin=1.0):\n",
    "\n",
    "    # Get the shape of hidden_states: [seq_len, batch_size, hidden_size]\n",
    "    seq_len, batch_size, H = hidden_states.shape\n",
    "    loss_syn, loss_ant = 0.0, 0.0  # Initialize the synonym and antonym loss\n",
    "    count_syn, count_ant = 0, 0  # Initialize the counters for synonyms and antonyms\n",
    "\n",
    "    # Transpose hidden_states to [batch_size, seq_len, hidden_size] for easy indexing\n",
    "    hs = hidden_states.transpose(0, 1)\n",
    "\n",
    "    # Iterate through the batch (each example)\n",
    "    for b in range(batch_size):\n",
    "        tokens = [vocab.itos[i] for i in input_ids[:, b].tolist()]  # Get the tokens for this example\n",
    "\n",
    "        # Iterate through each token to find synonyms and antonyms\n",
    "        for i, tok in enumerate(tokens):\n",
    "            # Get the list of synonyms and antonyms for the current token from the lexicon\n",
    "            syns = lexicon.get_synonyms(tok)\n",
    "            ants = lexicon.get_antonyms(tok)\n",
    "\n",
    "            if syns:\n",
    "                # For each synonym, calculate the Mean Squared Error (MSE) loss between the token and the synonym's embeddings\n",
    "                for j, tok2 in enumerate(tokens):\n",
    "                    if tok2 in syns:  # If the token is a synonym\n",
    "                        v1, v2 = hs[b, i], hs[b, j]  # Get the embeddings for the token and its synonym\n",
    "                        loss_syn += F.mse_loss(v1, v2)  # Add the MSE loss to the total synonym loss\n",
    "                        count_syn += 1  # Count the number of synonym pairs\n",
    "                        break  # Only consider the first matching synonym in the sentence\n",
    "\n",
    "            if ants:\n",
    "                # For each antonym, calculate the margin-based loss to increase the distance between antonyms' embeddings\n",
    "                for j, tok2 in enumerate(tokens):\n",
    "                    if tok2 in ants:  # If the token is an antonym\n",
    "                        v1, v2 = hs[b, i], hs[b, j]  # Get the embeddings for the token and its antonym\n",
    "                        dist = F.pairwise_distance(v1.unsqueeze(0), v2.unsqueeze(0))\n",
    "                        loss_ant += torch.clamp(margin - dist, min=0.0).mean()  # Add the margin loss to the antonym loss\n",
    "                        count_ant += 1  # Count the number of antonym pairs\n",
    "                        break  # Only consider the first matching antonym in the sentence\n",
    "\n",
    "    # Normalize the loss by the number of synonym and antonym pairs found\n",
    "    if count_syn > 0:\n",
    "        loss_syn = loss_syn / count_syn\n",
    "    if count_ant > 0:\n",
    "        loss_ant = loss_ant / count_ant\n",
    "\n",
    "    # Return the weighted sum of synonym and antonym losses\n",
    "    return lamda * (loss_syn + loss_ant)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training and Evaluation Routines\n",
    "This section defines two core functions for model training and validation in the single-sentence classification task:\n",
    "\n",
    "1. **Evaluation Function** (`evaluate`):\n",
    "   - Switches model to inference mode with `model.eval()`, disabling dropout and gradient computation.\n",
    "   - Iterates over `data_iter`, moves inputs (`x`) and labels (`y`) to the specified `device`.\n",
    "   - Constructs a **padding mask** via `(x == PAD_IDX).transpose(0, 1)`, marking padded positions so they do not affect attention.\n",
    "   - Computes `logits = model(x, attention_mask=padding_mask)`, selects the highest-scoring classes, and accumulates correct predictions.\n",
    "   - Returns overall accuracy as `acc_sum / n`.\n",
    "\n",
    "2. **Training Function** (`train`):\n",
    "   - Instantiates `BertForSentenceClassification` with `config` and optionally loads an existing checkpoint from `model_save_path`.\n",
    "   - Moves the model to `device` and sets it to training mode (`model.train()`).\n",
    "   - Initializes an `Adam` optimizer on model parameters with a learning rate of 5e-5.\n",
    "   - Creates a `LoadSingleSentenceClassificationDataset` loader and obtains `train_iter`, `val_iter`, and `test_iter`.\n",
    "   - Loops over epochs:\n",
    "     - For each batch, moves data to `device`, builds the padding mask, and calls `model(..., labels=label)`, which returns `(loss, logits)`.\n",
    "     - Performs backpropagation (`loss.backward()`) and optimization step (`optimizer.step()`).\n",
    "     - Accumulates batch losses to compute the epoch’s average loss.\n",
    "   - Every `config.model_val_per_epoch` epochs, calls `evaluate(val_iter, ...)` to compute validation accuracy; if improved, saves the model state.\n",
    "\n",
    "These routines encapsulate the full training lifecycle, from data loading and forward/backward passes to checkpointing based on validation performance.\n",
    "\n",
    "#### PS:The training time is too long to here,please go to Task/TaskForSingleSentenceClassification.py,.pt model has saved to /cache"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "from model.HowNet import HowNetLexicon\n",
    "import logging\n",
    "import time\n",
    "from utils import LoadSingleSentenceClassificationDataset\n",
    "from model import BertForSentenceClassification\n",
    "lexicon = HowNetLexicon()\n",
    "λ_sem = 0.2\n",
    "def evaluate(data_iter, model, device, PAD_IDX):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        acc_sum, n = 0.0, 0\n",
    "        for x, y in data_iter:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            padding_mask = (x == PAD_IDX).transpose(0, 1)\n",
    "            logits = model(x, attention_mask=padding_mask)\n",
    "            acc_sum += (logits.argmax(1) == y).float().sum().item()\n",
    "            n += len(y)\n",
    "        model.train()\n",
    "        return acc_sum / n\n",
    "def train(config,λ_sem=0.1):\n",
    "    model = BertForSentenceClassification(config,bert_pretrained_model_dir=None)\n",
    "    model_save_path = os.path.join(config.model_save_dir, 'model.pt')\n",
    "    if os.path.exists(model_save_path):\n",
    "        loaded_paras = torch.load(model_save_path,map_location=torch.device('cpu'))\n",
    "        model.load_state_dict(loaded_paras)\n",
    "        logging.info(\"## load model......\")\n",
    "    model = model.to(config.device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "    model.train()\n",
    "    bert_tokenize = BertTokenizer.from_pretrained(config.pretrained_model_dir).tokenize\n",
    "    data_loader = LoadSingleSentenceClassificationDataset(vocab_path=config.vocab_path,\n",
    "                                                          tokenizer=bert_tokenize,\n",
    "                                                          batch_size=config.batch_size,\n",
    "                                                          max_sen_len=config.max_sen_len,\n",
    "                                                          split_sep=config.split_sep,\n",
    "                                                          max_position_embeddings=config.max_position_embeddings,\n",
    "                                                          pad_index=config.pad_token_id,\n",
    "                                                          is_sample_shuffle=config.is_sample_shuffle)\n",
    "    train_iter, test_iter, val_iter = data_loader.load_train_val_test_data(config.train_file_path,\n",
    "                                                                           config.val_file_path,\n",
    "                                                                           config.test_file_path)\n",
    "    max_acc = 0\n",
    "    for epoch in range(config.epochs):\n",
    "        losses = 0\n",
    "        start_time = time.time()\n",
    "        for idx, (sample, label) in enumerate(train_iter):\n",
    "            sample = sample.to(config.device)  # [src_len, batch_size]\n",
    "            label = label.to(config.device)\n",
    "            padding_mask = (sample == data_loader.PAD_IDX).transpose(0, 1)\n",
    "            loss, logits = model(\n",
    "                input_ids=sample,\n",
    "                attention_mask=padding_mask,\n",
    "                token_type_ids=None,\n",
    "                position_ids=None,\n",
    "                labels=label)\n",
    "            sem_loss = compute_semantic_loss(\n",
    "                input_ids=sample,\n",
    "                hidden_states=torch.zeros([512,200,768]),  # Last hidden layer [seq_len, batch_size, hidden_size]\n",
    "                vocab=data_loader.vocab,\n",
    "                lexicon=lexicon,\n",
    "                lamda=λ_sem\n",
    "            )\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses += loss.item()+sem_loss*λ_sem\n",
    "            acc = (logits.argmax(1) == label).float().mean()\n",
    "            if idx % 10 == 0:\n",
    "                logging.info(f\"Epoch: {epoch}, Batch[{idx}/{len(train_iter)}], \"\n",
    "                             f\"Train loss :{loss.item():.3f}, Train acc: {acc:.3f}\")\n",
    "        end_time = time.time()\n",
    "        train_loss = losses / len(train_iter)\n",
    "        logging.info(f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Epoch time = {(end_time - start_time):.3f}s\")\n",
    "        if (epoch + 1) % config.model_val_per_epoch == 0:\n",
    "            acc = evaluate(val_iter, model, config.device, data_loader.PAD_IDX)\n",
    "            logging.info(f\"Accuracy on val {acc:.3f}\")\n",
    "            if acc > max_acc:\n",
    "                max_acc = acc\n",
    "                torch.save(model.state_dict(), model_save_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prediction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer,pipeline\n",
    "_mapping_txt = \"\"\"\\\n",
    "100 民生 故事 news_story\n",
    "101 文化 文化 news_culture\n",
    "102 娱乐 娱乐 news_entertainment\n",
    "103 体育 体育 news_sports\n",
    "104 财经 财经 news_finance\n",
    "106 房产 房产 news_house\n",
    "107 汽车 汽车 news_car\n",
    "108 教育 教育 news_edu\n",
    "109 科技 科技 news_tech\n",
    "110 军事 军事 news_military\n",
    "112 旅游 旅游 news_travel\n",
    "113 国际 国际 news_world\n",
    "114 证券 股票 stock\n",
    "115 农业 三农 news_agriculture\n",
    "116 电竞 游戏 news_game\n",
    "\"\"\"\n",
    "_category_map = []\n",
    "for line in _mapping_txt.splitlines():\n",
    "    code, zh1, zh2, en = line.strip().split()\n",
    "    _category_map.append({\n",
    "        \"code\": code,\n",
    "        \"zh\"  : zh1 + zh2,\n",
    "        \"en\"  : en\n",
    "    })\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-zh-en\")\n",
    "\n",
    "def predict_and_translate(sentence: str):\n",
    "    config = ModelConfig()\n",
    "    device = config.device\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(config.pretrained_model_dir)\n",
    "    loader = LoadSingleSentenceClassificationDataset(\n",
    "        vocab_path=config.vocab_path,\n",
    "        tokenizer=tokenizer.tokenize,\n",
    "        batch_size=1,\n",
    "        max_sen_len=config.max_sen_len,\n",
    "        split_sep=config.split_sep,\n",
    "        max_position_embeddings=config.max_position_embeddings,\n",
    "        pad_index=config.pad_token_id,\n",
    "        is_sample_shuffle=False\n",
    "    )\n",
    "\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    ids = [loader.CLS_IDX] + [\n",
    "        loader.vocab.stoi.get(t, loader.vocab.stoi[loader.vocab.UNK])\n",
    "        for t in tokens\n",
    "    ] + [loader.SEP_IDX]\n",
    "    max_len = loader.max_position_embeddings - 1\n",
    "    if len(ids) > max_len + 1:\n",
    "        ids = ids[:max_len] + [loader.SEP_IDX]\n",
    "\n",
    "    tensor = torch.tensor(ids, dtype=torch.long)\n",
    "    batch_sent, _ = loader.generate_batch([(tensor, 0)])\n",
    "    batch_sent = batch_sent.to(device)\n",
    "    attention_mask = (batch_sent == loader.PAD_IDX).transpose(0, 1)\n",
    "    model = BertForSentenceClassification(config, config.pretrained_model_dir)\n",
    "    state_dict = torch.load(\n",
    "        os.path.join(config.model_save_dir, 'model.pt'),\n",
    "        map_location=device\n",
    "    )\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device).eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(batch_sent, attention_mask=attention_mask)\n",
    "        idx = logits.argmax(dim=1).item()\n",
    "    item = _category_map[idx]\n",
    "    class_result = f\"{item['zh']} （{item['en']}，code={item['code']}）\"\n",
    "\n",
    "    translation = translator(sentence, max_length=256)[0][\"translation_text\"]\n",
    "\n",
    "    return class_result, translation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title is \n",
      "Prediction and translation ('文化文化 （news_culture，code=101）', \"I don't think so.\")\n"
     ]
    }
   ],
   "source": [
    "s = input(\"article title: \\n\")\n",
    "print(\"Title is\",s)\n",
    "print(\"Prediction and translation\", predict_and_translate(s))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
